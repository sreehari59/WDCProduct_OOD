{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62273701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from itertools import combinations\n",
    "import math\n",
    "\n",
    "from gensim.parsing.preprocessing import lower_to_unicode, preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric\n",
    "\n",
    "import py_stringmatching as sm\n",
    "from gensim.models import FastText\n",
    "from copy import deepcopy\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52012152",
   "metadata": {},
   "source": [
    "# Load pretrained fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093c8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTTEXT_MODEL = FastText.load('../../../models/fasttext/deepmatcher_product_datasets.model').wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493ee65",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that selects hard negatives in DBSCAN clusters using SoftTFIDF and Cosine similarity\n",
    "def select_cc_clusters(corpus_dbscan, cc_candidates_seen):\n",
    "    \n",
    "    corpus_dbscan = corpus_dbscan.set_index('cluster_id', drop=False).copy()\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces]\n",
    "    \n",
    "    selection_final = []\n",
    "    selection_final_tupled = []\n",
    "    #amounts = [400, 250, 100]\n",
    "    large = 400\n",
    "    medium = 250\n",
    "    small = 100\n",
    "    \n",
    "    random.shuffle(cc_candidates_seen)\n",
    "    \n",
    "\n",
    "    sub_corpus_dbscan = corpus_dbscan.copy()\n",
    "    cur_selection = set()\n",
    "    cur_selection_tupled = set()\n",
    "\n",
    "    while len(cur_selection) < large:\n",
    "        for dbscan_id in cc_candidates_seen:\n",
    "            sub_corpus = sub_corpus_dbscan[sub_corpus_dbscan['dbscan_cluster'] == dbscan_id].drop_duplicates('cluster_id').copy()\n",
    "            if len(sub_corpus) < 5:\n",
    "                continue\n",
    "\n",
    "            sub_corpus['title_processed'] = sub_corpus['title'].apply(lower_to_unicode)\n",
    "            sub_corpus['title_processed'] = sub_corpus['title_processed'].apply(preprocess_string, args=(CUSTOM_FILTERS,))\n",
    "\n",
    "            similarities=[#sm.SoftTfIdf(corpus_list=sub_corpus['title_processed'].tolist(), threshold=0.7), \n",
    "                          sm.Cosine(), \n",
    "                         sm.GeneralizedJaccard(threshold=0.7),\n",
    "                         sm.Dice(),\n",
    "                         FASTTEXT_MODEL]\n",
    "\n",
    "            sub_cids = sorted(list(set(sub_corpus['cluster_id'])))\n",
    "            random_pick = random.sample(sub_cids, 1)\n",
    "            similarities_results = []\n",
    "            sub_selection = set()\n",
    "\n",
    "            example = sub_corpus.loc[random_pick]['title_processed'].iloc[0]\n",
    "            sub_corpus = sub_corpus.drop(random_pick)\n",
    "\n",
    "            for sim in similarities:\n",
    "                try:\n",
    "                    result = sub_corpus['title_processed'].apply(lambda x: sim.get_sim_score(example, x))\n",
    "                except AttributeError:\n",
    "                    try:\n",
    "                        result = sub_corpus['title_processed'].apply(lambda x: sim.get_raw_score(example, x))\n",
    "                    except AttributeError:\n",
    "                        result = sub_corpus['title_processed'].apply(lambda x: sim.n_similarity(example, x))\n",
    "                \n",
    "                similarities_results.append(result)\n",
    "            sorted_sim = [x.sort_values(ascending=False) for x in similarities_results]\n",
    "\n",
    "            while len(sub_selection) <= 3:\n",
    "                index = random.sample(range(4), 1)[0]\n",
    "                found = False\n",
    "                counter = 0\n",
    "                while not found:\n",
    "                    selected = sorted_sim[index].index[counter]\n",
    "                    if selected not in sub_selection:\n",
    "                        found = True\n",
    "                    counter += 1\n",
    "                sub_selection.add(selected)\n",
    "\n",
    "            sub_selection.update(random_pick)\n",
    "            cur_selection.update(sub_selection)\n",
    "            cur_selection_tupled.add(tuple(sub_selection))\n",
    "\n",
    "            sub_corpus_dbscan = sub_corpus_dbscan.loc[~sub_corpus_dbscan.index.isin(cur_selection)]\n",
    "            if len(cur_selection) == small:\n",
    "                cur_selection_small = deepcopy(cur_selection)\n",
    "                cur_selection_tupled_small = deepcopy(cur_selection_tupled)\n",
    "            if len(cur_selection) == medium:\n",
    "                cur_selection_medium = deepcopy(cur_selection)\n",
    "                cur_selection_tupled_medium = deepcopy(cur_selection_tupled)\n",
    "            if len(cur_selection) == large:\n",
    "                cur_selection_large = cur_selection\n",
    "                cur_selection_tupled_large = deepcopy(cur_selection_tupled)\n",
    "                break\n",
    "\n",
    "    selection_final.append(cur_selection_large)\n",
    "    selection_final.append(cur_selection_medium)\n",
    "    selection_final.append(cur_selection_small)\n",
    "    selection_final_tupled.append(cur_selection_tupled_large)\n",
    "    selection_final_tupled.append(cur_selection_tupled_medium)\n",
    "    selection_final_tupled.append(cur_selection_tupled_small)\n",
    "        \n",
    "    return selection_final_tupled\n",
    "\n",
    "# function that selects random clusters\n",
    "def select_rnd_clusters(corpus_dbscan, ccs, cc_candidates_seen):\n",
    "\n",
    "    ccs_set = set()\n",
    "    ccs_set.update(*ccs)\n",
    "\n",
    "    corpus_dbscan = corpus_dbscan[~corpus_dbscan['cluster_id'].isin(ccs_set)].copy()\n",
    "    corpus_dbscan = corpus_dbscan[corpus_dbscan['dbscan_cluster'].isin(cc_candidates_seen)]\n",
    "    \n",
    "    selection_final = []\n",
    "    #amounts = [400, 250, 100]\n",
    "    large = 400\n",
    "    medium = 250\n",
    "    small = 100\n",
    "    \n",
    "    sub_corpus = corpus_dbscan.set_index('cluster_id', drop=False).copy()\n",
    "\n",
    "    #counts = sub_corpus['cluster_id'].value_counts()\n",
    "    #counts = counts[counts >6]\n",
    "    #sub_corpus = sub_corpus[sub_corpus['cluster_id'].isin(counts.index)]\n",
    "\n",
    "    sub_corpus = sub_corpus.sort_values('id')\n",
    "    sub_corpus = sub_corpus.set_index('id', drop=False)\n",
    "    sub_corpus = sub_corpus.drop_duplicates('cluster_id')\n",
    "    rnd_cids = sorted(list(set(sub_corpus['cluster_id'])))\n",
    "\n",
    "    sample = random.sample(rnd_cids, large)\n",
    "    \n",
    "    sub_sample_large = sample[:large]\n",
    "    sub_sample_medium = sub_sample_large[:medium]\n",
    "    sub_sample_small = sub_sample_medium[:small]\n",
    "    sub_sample_large = set(sub_sample_large)\n",
    "    sub_sample_medium = set(sub_sample_medium)\n",
    "    sub_sample_small = set(sub_sample_small)\n",
    "    \n",
    "    selection_final.append(sub_sample_large)\n",
    "    selection_final.append(sub_sample_medium)\n",
    "    selection_final.append(sub_sample_small)\n",
    "    \n",
    "    return selection_final\n",
    "\n",
    "# funtion to build pairs from the created train, validation and test splits\n",
    "def build_pairs(corpus, ccs, rnd, ccs_unseen, rnd_unseen, ccs_val, rnd_val):\n",
    "    \n",
    "    counting = 0\n",
    "    \n",
    "    # seen\n",
    "    ccs_set = set()\n",
    "    ccs_set.update(*ccs)\n",
    "    \n",
    "    combined_cids = list(ccs_set | rnd)\n",
    "    corpus_dbscan = corpus.sort_values('id').copy()\n",
    "    corpus_dbscan = corpus_dbscan.set_index('cluster_id', drop=False)\n",
    "    \n",
    "    corpus_dbscan = corpus_dbscan.loc[combined_cids]\n",
    "    corpus_dbscan = corpus_dbscan.set_index('id', drop=False)\n",
    "    \n",
    "    sample_to_10 = set()\n",
    "    test_ids = set()\n",
    "    valid_ids = set()\n",
    "    train_ids = set()\n",
    "    \n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces]\n",
    "    \n",
    "    \n",
    "    \n",
    "    corpus_dbscan['title_processed'] = corpus_dbscan['title'].apply(lower_to_unicode)\n",
    "    corpus_dbscan['title_processed'] = corpus_dbscan['title_processed'].apply(preprocess_string, args=(CUSTOM_FILTERS,))\n",
    "    \n",
    "    similarities=[#sm.SoftTfIdf(corpus_list=corpus_dbscan['title_processed'].tolist(), threshold=0.7),\n",
    "                          sm.Cosine(), \n",
    "                         sm.GeneralizedJaccard(threshold=0.7),\n",
    "                         sm.Dice(),\n",
    "                         FASTTEXT_MODEL]\n",
    "    \n",
    "    for name, group in corpus_dbscan.groupby('cluster_id'):\n",
    "        \n",
    "        \n",
    "        \n",
    "        if len(group) > 7:\n",
    "            max_len = len(group)\n",
    "            if max_len < 15:\n",
    "                cur_10 = sorted(list(set(group.sample(max_len)['id'])))\n",
    "            else:\n",
    "                if len(group) > 15:\n",
    "                    counting +=1\n",
    "                cur_10 = sorted(list(set(group.sample(15)['id'])))\n",
    "        else:\n",
    "            cur_10 = sorted(list(set(group.sample(7)['id'])))\n",
    "        sample_to_10.update(cur_10)\n",
    "        \n",
    "        group = group[group['id'].isin(sample_to_10)].copy()\n",
    "        group_ids = group['id']\n",
    "        combs = list(combinations(group_ids, 2))\n",
    "        sims = []\n",
    "        for simno, sim in enumerate(similarities):\n",
    "            cur_res = []\n",
    "            for combination in combs:\n",
    "                try:\n",
    "                    result = sim.get_sim_score(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                except AttributeError:\n",
    "                    try:\n",
    "                        result = sim.get_raw_score(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                    except AttributeError:\n",
    "                        result = sim.n_similarity(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                cur_res.append(result)\n",
    "            sims.append(cur_res)\n",
    "        #averaged = [sum(i)/len(similarities) for i in zip(*sims)]\n",
    "        \n",
    "        tupled = set()\n",
    "        for sim in sims:\n",
    "            tupled.update([(c, sim[i]) for i, c in enumerate(combs)])\n",
    "                  \n",
    "        sorted_sim = sorted(list(tupled), key = lambda y: y[1])\n",
    "        sorted_sim = [x[0] for x in sorted_sim]\n",
    "        check_cc_set = set()\n",
    "        for cur_set in ccs:\n",
    "            check_cc_set.update(cur_set)\n",
    "        is_cc = name in check_cc_set\n",
    "        \n",
    "        if is_cc:\n",
    "            cc_bucket = sorted_sim[:(math.ceil(len(sorted_sim)/5))]\n",
    "            rest_bucket = sorted_sim[(math.ceil(len(sorted_sim)/5)):]\n",
    "            \n",
    "            test_ids.update(set(*random.sample(cc_bucket, 1)))\n",
    "            cc_bucket = [x for x in cc_bucket if x[0] not in test_ids and x[1] not in test_ids] \n",
    "            try:\n",
    "                valid_ids.update(set(*random.sample(cc_bucket, 1)))\n",
    "            except ValueError:\n",
    "                valid_ids.update(set(*random.sample(rest_bucket, 1)))\n",
    "            remaining = set(group_ids) - test_ids - valid_ids\n",
    "            train_ids.update(remaining)\n",
    "\n",
    "        else:\n",
    "            test_ids.update(set(random.sample(cur_10, 2)))\n",
    "            remaining = sorted(list(set(cur_10) - test_ids))\n",
    "            valid_ids.update(set(random.sample(remaining, 2)))\n",
    "            remaining = set(remaining) - valid_ids\n",
    "            train_ids.update(remaining)\n",
    "\n",
    "    assert len(test_ids & valid_ids & train_ids) == 0\n",
    "\n",
    "    corpus_dbscan = corpus_dbscan[corpus_dbscan['id'].isin(sample_to_10)]\n",
    "\n",
    "    # unseen 50\n",
    "    ccs_seen_50 = random.sample(list(ccs), math.ceil(len(ccs)/2))\n",
    "    rnd_seen_50 = random.sample(list(rnd), math.ceil(len(rnd)/2))\n",
    "    ccs_unseen_50 = random.sample(list(ccs_unseen), math.ceil(len(ccs_unseen)/2))\n",
    "    rnd_unseen_50 = random.sample(list(rnd_unseen), math.ceil(len(rnd_unseen)/2))\n",
    "    \n",
    "    ccs_val_50 = random.sample(list(ccs_val), math.ceil(len(ccs_val)/2))\n",
    "    rnd_val_50 = random.sample(list(rnd_val), math.ceil(len(rnd_val)/2))\n",
    "    \n",
    "    ccs_seen_50_set = set()\n",
    "    ccs_seen_50_set.update(*ccs_seen_50)\n",
    "    rnd_seen_50_set = set()\n",
    "    rnd_seen_50_set.update(rnd_seen_50)\n",
    "    ccs_unseen_50_set = set()\n",
    "    ccs_unseen_50_set.update(*ccs_unseen_50)\n",
    "    rnd_unseen_50_set = set()\n",
    "    rnd_unseen_50_set.update(rnd_unseen_50)\n",
    "    \n",
    "    ccs_val_50_set = set()\n",
    "    ccs_val_50_set.update(*ccs_val_50)\n",
    "    rnd_val_50_set = set()\n",
    "    rnd_val_50_set.update(rnd_val_50)\n",
    "    \n",
    "    combined_cids_unseen_50 = ccs_seen_50_set | rnd_seen_50_set | ccs_unseen_50_set | rnd_unseen_50_set\n",
    "    corpus_dbscan_unseen_50 = corpus.sort_values('id').copy()\n",
    "    corpus_dbscan_unseen_50 = corpus_dbscan_unseen_50.set_index('cluster_id', drop=False)\n",
    "    \n",
    "    corpus_dbscan_unseen_50 = corpus_dbscan_unseen_50.loc[combined_cids_unseen_50]\n",
    "    corpus_dbscan_unseen_50 = corpus_dbscan_unseen_50.set_index('id', drop=False)\n",
    "    \n",
    "    sample_to_3_unseen_50 = set()\n",
    "    test_ids_unseen_50 = set()\n",
    "    \n",
    "    corpus_dbscan_unseen_50['title_processed'] = corpus_dbscan_unseen_50['title'].apply(lower_to_unicode)\n",
    "    corpus_dbscan_unseen_50['title_processed'] = corpus_dbscan_unseen_50['title_processed'].apply(preprocess_string, args=(CUSTOM_FILTERS,))\n",
    "    \n",
    "    similarities=[#sm.SoftTfIdf(corpus_list=corpus_dbscan_unseen_50['title_processed'].tolist(), threshold=0.7),\n",
    "                          sm.Cosine(), \n",
    "                         sm.GeneralizedJaccard(threshold=0.7),\n",
    "                         sm.Dice(),\n",
    "                         FASTTEXT_MODEL]\n",
    "    \n",
    "    for name, group in corpus_dbscan_unseen_50.groupby('cluster_id'):\n",
    "        \n",
    "        group_ids = group['id']\n",
    "        combs = list(combinations(group_ids, 2))\n",
    "        sims = []\n",
    "        for simno, sim in enumerate(similarities):\n",
    "            cur_res = []\n",
    "            for combination in combs:\n",
    "                try:\n",
    "                    result = sim.get_sim_score(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                except AttributeError:\n",
    "                    try:\n",
    "                        result = sim.get_raw_score(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                    except AttributeError:\n",
    "                        result = sim.n_similarity(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                cur_res.append(result)\n",
    "            sims.append(cur_res)\n",
    "        \n",
    "        tupled = set()\n",
    "        for sim in sims:\n",
    "            tupled.update([(c, sim[i]) for i, c in enumerate(combs)])\n",
    "                  \n",
    "        sorted_sim = sorted(list(tupled), key = lambda y: y[1])\n",
    "        sorted_sim = [x[0] for x in sorted_sim]\n",
    "        check_cc_set = set()\n",
    "        for cur_set in ccs_unseen:\n",
    "            check_cc_set.update(cur_set)\n",
    "        is_cc = name in check_cc_set\n",
    "        \n",
    "        if is_cc:\n",
    "            \n",
    "            cc_bucket = sorted_sim[:(math.ceil(len(sorted_sim)/5))]\n",
    "            rest_bucket = sorted_sim[(math.ceil(len(sorted_sim)/5)):]\n",
    "            \n",
    "            cur_3_unseen_50 = set(*random.sample(cc_bucket, 1))\n",
    "            sample_to_3_unseen_50.update(cur_3_unseen_50)\n",
    "        else:\n",
    "            cur_3_unseen_50 = sorted(list(set(group.sample(2)['id'])))\n",
    "            sample_to_3_unseen_50.update(cur_3_unseen_50)\n",
    "        \n",
    "#         cur_3_unseen_50 = sorted(list(set(group.sample(2)['id'])))\n",
    "#         sample_to_3_unseen_50.update(cur_3_unseen_50)\n",
    "        \n",
    "        test_ids_unseen_50.update(sample_to_3_unseen_50)\n",
    "\n",
    "    corpus_dbscan_unseen_50 = corpus_dbscan_unseen_50[corpus_dbscan_unseen_50['id'].isin(sample_to_3_unseen_50)]\n",
    "    \n",
    "    combined_cids_val_50 = ccs_seen_50_set | rnd_seen_50_set | ccs_val_50_set | rnd_val_50_set\n",
    "    corpus_dbscan_val_50 = corpus.sort_values('id').copy()\n",
    "    corpus_dbscan_val_50 = corpus_dbscan_val_50.set_index('cluster_id', drop=False)\n",
    "    \n",
    "    corpus_dbscan_val_50 = corpus_dbscan_val_50.loc[combined_cids_val_50]\n",
    "    corpus_dbscan_val_50 = corpus_dbscan_val_50.set_index('id', drop=False)\n",
    "    \n",
    "    sample_to_3_val_50 = set()\n",
    "    test_ids_val_50 = set()\n",
    "    \n",
    "    corpus_dbscan_val_50['title_processed'] = corpus_dbscan_val_50['title'].apply(lower_to_unicode)\n",
    "    corpus_dbscan_val_50['title_processed'] = corpus_dbscan_val_50['title_processed'].apply(preprocess_string, args=(CUSTOM_FILTERS,))\n",
    "    \n",
    "    similarities=[#sm.SoftTfIdf(corpus_list=corpus_dbscan_val_50['title_processed'].tolist(), threshold=0.7),\n",
    "                          sm.Cosine(), \n",
    "                         sm.GeneralizedJaccard(threshold=0.7),\n",
    "                         sm.Dice(),\n",
    "                         FASTTEXT_MODEL]\n",
    "    \n",
    "    for name, group in corpus_dbscan_val_50.groupby('cluster_id'):\n",
    "        \n",
    "        group_ids = group['id']\n",
    "        combs = list(combinations(group_ids, 2))\n",
    "        sims = []\n",
    "        for simno, sim in enumerate(similarities):\n",
    "            cur_res = []\n",
    "            for combination in combs:\n",
    "                try:\n",
    "                    result = sim.get_sim_score(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                except AttributeError:\n",
    "                    try:\n",
    "                        result = sim.get_raw_score(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                    except AttributeError:\n",
    "                        result = sim.n_similarity(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                cur_res.append(result)\n",
    "            sims.append(cur_res)\n",
    "            \n",
    "        tupled = set()\n",
    "        for sim in sims:\n",
    "            tupled.update([(c, sim[i]) for i, c in enumerate(combs)])\n",
    "                  \n",
    "        sorted_sim = sorted(list(tupled), key = lambda y: y[1])\n",
    "        sorted_sim = [x[0] for x in sorted_sim]\n",
    "        check_cc_set = set()\n",
    "        for cur_set in ccs_val:\n",
    "            check_cc_set.update(cur_set)\n",
    "        is_cc = name in check_cc_set\n",
    "        if is_cc:\n",
    "            \n",
    "            cc_bucket = sorted_sim[:(math.ceil(len(sorted_sim)/5))]\n",
    "            rest_bucket = sorted_sim[(math.ceil(len(sorted_sim)/5)):]\n",
    "            \n",
    "            cur_3_val_50 = set(*random.sample(cc_bucket, 1))\n",
    "            sample_to_3_val_50.update(cur_3_val_50)\n",
    "            \n",
    "        else:\n",
    "            cur_3_val_50 = sorted(list(set(group.sample(2)['id'])))\n",
    "            sample_to_3_val_50.update(cur_3_val_50)\n",
    "        \n",
    "#         cur_3_val_50 = sorted(list(set(group.sample(2)['id'])))\n",
    "#         sample_to_3_val_50.update(cur_3_val_50)\n",
    "        \n",
    "        test_ids_val_50.update(sample_to_3_val_50)\n",
    "\n",
    "    corpus_dbscan_val_50 = corpus_dbscan_val_50[corpus_dbscan_val_50['id'].isin(sample_to_3_val_50)]\n",
    "    \n",
    "     # unseen 100\n",
    "    ccs_unseen_set = set()\n",
    "    ccs_unseen_set.update(*ccs_unseen)\n",
    "    \n",
    "    combined_cids_unseen_100 = list(ccs_unseen_set | rnd_unseen)\n",
    "    corpus_dbscan_unseen_100 = corpus.sort_values('id').copy()\n",
    "    corpus_dbscan_unseen_100 = corpus_dbscan_unseen_100.set_index('cluster_id', drop=False)\n",
    "    \n",
    "    corpus_dbscan_unseen_100 = corpus_dbscan_unseen_100.loc[combined_cids_unseen_100]\n",
    "    corpus_dbscan_unseen_100 = corpus_dbscan_unseen_100.set_index('id', drop=False)\n",
    "    \n",
    "    sample_to_3_unseen_100 = set()\n",
    "    test_ids_unseen_100 = set()\n",
    "    \n",
    "    corpus_dbscan_unseen_100['title_processed'] = corpus_dbscan_unseen_100['title'].apply(lower_to_unicode)\n",
    "    corpus_dbscan_unseen_100['title_processed'] = corpus_dbscan_unseen_100['title_processed'].apply(preprocess_string, args=(CUSTOM_FILTERS,))\n",
    "    \n",
    "    similarities=[#sm.SoftTfIdf(corpus_list=corpus_dbscan_unseen_100['title_processed'].tolist(), threshold=0.7),\n",
    "                          sm.Cosine(), \n",
    "                         sm.GeneralizedJaccard(threshold=0.7),\n",
    "                         sm.Dice(),\n",
    "                         FASTTEXT_MODEL]\n",
    "    \n",
    "    budget_cc = 0\n",
    "    budgetrnd = 0\n",
    "    \n",
    "    for name, group in corpus_dbscan_unseen_100.groupby('cluster_id'):\n",
    "        \n",
    "        group_ids = group['id']\n",
    "        combs = list(combinations(group_ids, 2))\n",
    "        sims = []\n",
    "        for simno, sim in enumerate(similarities):\n",
    "            cur_res = []\n",
    "            for combination in combs:\n",
    "                try:\n",
    "                    result = sim.get_sim_score(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                except AttributeError:\n",
    "                    try:\n",
    "                        result = sim.get_raw_score(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                    except AttributeError:\n",
    "                        result = sim.n_similarity(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                cur_res.append(result)\n",
    "            sims.append(cur_res)\n",
    "            \n",
    "        tupled = set()\n",
    "        for sim in sims:\n",
    "            tupled.update([(c, sim[i]) for i, c in enumerate(combs)])\n",
    "                  \n",
    "        sorted_sim = sorted(list(tupled), key = lambda y: y[1])\n",
    "        sorted_sim = [x[0] for x in sorted_sim]\n",
    "        check_cc_set = set()\n",
    "        for cur_set in ccs_unseen:\n",
    "            check_cc_set.update(cur_set)\n",
    "        is_cc = name in check_cc_set\n",
    "        if is_cc:\n",
    "            \n",
    "            cc_bucket = sorted_sim[:(math.ceil(len(sorted_sim)/5))]\n",
    "            rest_bucket = sorted_sim[(math.ceil(len(sorted_sim)/5)):]\n",
    "            \n",
    "            cur_3_unseen_100 = set(*random.sample(cc_bucket, 1))\n",
    "            sample_to_3_unseen_100.update(cur_3_unseen_100)\n",
    "\n",
    "        else:\n",
    "            cur_3_unseen_100 = sorted(list(set(group.sample(2)['id'])))\n",
    "            sample_to_3_unseen_100.update(cur_3_unseen_100)\n",
    "        \n",
    "#         cur_3_unseen_100 = sorted(list(set(group.sample(2)['id'])))\n",
    "#         sample_to_3_unseen_100.update(cur_3_unseen_100)\n",
    "        \n",
    "        test_ids_unseen_100.update(sample_to_3_unseen_100)\n",
    "\n",
    "    corpus_dbscan_unseen_100 = corpus_dbscan_unseen_100[corpus_dbscan_unseen_100['id'].isin(sample_to_3_unseen_100)]\n",
    "    \n",
    "    ccs_val_set = set()\n",
    "    ccs_val_set.update(*ccs_unseen)\n",
    "    \n",
    "    combined_cids_val_100 = list(ccs_val_set | rnd_val)\n",
    "    corpus_dbscan_val_100 = corpus.sort_values('id').copy()\n",
    "    corpus_dbscan_val_100 = corpus_dbscan_val_100.set_index('cluster_id', drop=False)\n",
    "    \n",
    "    corpus_dbscan_val_100 = corpus_dbscan_val_100.loc[combined_cids_val_100]\n",
    "    corpus_dbscan_val_100 = corpus_dbscan_val_100.set_index('id', drop=False)\n",
    "    \n",
    "    sample_to_3_val_100 = set()\n",
    "    test_ids_val_100 = set()\n",
    "    \n",
    "    corpus_dbscan_val_100['title_processed'] = corpus_dbscan_val_100['title'].apply(lower_to_unicode)\n",
    "    corpus_dbscan_val_100['title_processed'] = corpus_dbscan_val_100['title_processed'].apply(preprocess_string, args=(CUSTOM_FILTERS,))\n",
    "    \n",
    "    similarities=[#sm.SoftTfIdf(corpus_list=corpus_dbscan_val_100['title_processed'].tolist(), threshold=0.7),\n",
    "                          sm.Cosine(), \n",
    "                         sm.GeneralizedJaccard(threshold=0.7),\n",
    "                         sm.Dice(),\n",
    "                         FASTTEXT_MODEL]\n",
    "    \n",
    "    budget_cc = 0\n",
    "    budgetrnd = 0\n",
    "    \n",
    "    for name, group in corpus_dbscan_val_100.groupby('cluster_id'):\n",
    "        \n",
    "        group_ids = group['id']\n",
    "        combs = list(combinations(group_ids, 2))\n",
    "        sims = []\n",
    "        for simno, sim in enumerate(similarities):\n",
    "            cur_res = []\n",
    "            for combination in combs:\n",
    "                try:\n",
    "                    result = sim.get_sim_score(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                except AttributeError:\n",
    "                    try:\n",
    "                        result = sim.get_raw_score(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                    except AttributeError:\n",
    "                        result = sim.n_similarity(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                cur_res.append(result)\n",
    "            sims.append(cur_res)\n",
    "        tupled = set()\n",
    "        for sim in sims:\n",
    "            tupled.update([(c, sim[i]) for i, c in enumerate(combs)])\n",
    "                  \n",
    "        sorted_sim = sorted(list(tupled), key = lambda y: y[1])\n",
    "        sorted_sim = [x[0] for x in sorted_sim]\n",
    "        check_cc_set = set()\n",
    "        for cur_set in ccs_val:\n",
    "            check_cc_set.update(cur_set)\n",
    "        is_cc = name in check_cc_set\n",
    "        if is_cc:\n",
    "            \n",
    "            cc_bucket = sorted_sim[:(math.ceil(len(sorted_sim)/5))]\n",
    "            rest_bucket = sorted_sim[(math.ceil(len(sorted_sim)/5)):]\n",
    "            \n",
    "            cur_3_val_100 = set(*random.sample(cc_bucket, 1))\n",
    "            sample_to_3_val_100.update(cur_3_val_100)\n",
    "            \n",
    "        else:\n",
    "            cur_3_val_100 = sorted(list(set(group.sample(2)['id'])))\n",
    "            sample_to_3_val_100.update(cur_3_val_100)\n",
    "        \n",
    "#         cur_3_val_100 = sorted(list(set(group.sample(2)['id'])))\n",
    "#         sample_to_3_val_100.update(cur_3_val_100)\n",
    "        \n",
    "        test_ids_val_100.update(sample_to_3_val_100)\n",
    "\n",
    "    corpus_dbscan_val_100 = corpus_dbscan_val_100[corpus_dbscan_val_100['id'].isin(sample_to_3_val_100)]\n",
    "    \n",
    "    \n",
    "    # build test, valid, train\n",
    "    test_set, test_ccs = build_test(corpus_dbscan[corpus_dbscan['id'].isin(test_ids)], test_ids)\n",
    "    try:\n",
    "        assert len(test_set) == 9* len(combined_cids)\n",
    "    except AssertionError:\n",
    "        set_trace()\n",
    "\n",
    "    test_set_unseen_50, test_ccs_50 = build_test(corpus_dbscan_unseen_50[corpus_dbscan_unseen_50['id'].isin(test_ids_unseen_50)], test_ids_unseen_50)\n",
    "    try:\n",
    "        assert len(test_set_unseen_50) == 9* len(combined_cids)\n",
    "    except AssertionError:\n",
    "        set_trace()\n",
    "        \n",
    "    test_set_unseen_100, test_ccs_100 = build_test(corpus_dbscan_unseen_100[corpus_dbscan_unseen_100['id'].isin(test_ids_unseen_100)], test_ids_unseen_100)\n",
    "    try:\n",
    "        assert len(test_set_unseen_100) == 9* len(combined_cids)\n",
    "    except AssertionError:\n",
    "        set_trace()\n",
    "        \n",
    "    print('Test set built')\n",
    "    \n",
    "    valid_small, valid_medium, valid_large, valid_ccs = build_train(corpus_dbscan[corpus_dbscan['id'].isin(valid_ids)], valid_ids, ccs, is_valid=True)\n",
    "    try:\n",
    "        assert len(valid_large) == 9* len(combined_cids) and len(valid_medium) == 7* len(combined_cids) and len(valid_small) == 5* len(combined_cids)\n",
    "    except AssertionError:\n",
    "        set_trace()\n",
    "        \n",
    "    valid_small_unseen_50, valid_medium_unseen_50, valid_large_unseen_50, valid_ccs_unseen_50 = build_train(corpus_dbscan_val_50[corpus_dbscan_val_50['id'].isin(test_ids_val_50)], test_ids_val_50, ccs | ccs_val, is_valid=True)\n",
    "    try:\n",
    "        assert len(valid_large_unseen_50) == 9* len(combined_cids) and len(valid_medium_unseen_50) == 7* len(combined_cids) and len(valid_small_unseen_50) == 5* len(combined_cids)\n",
    "    except AssertionError:\n",
    "        set_trace()\n",
    "        \n",
    "    valid_small_unseen_100, valid_medium_unseen_100, valid_large_unseen_100, valid_ccs_unseen_100 = build_train(corpus_dbscan_val_100[corpus_dbscan_val_100['id'].isin(test_ids_val_100)], test_ids_val_100, ccs | ccs_val, is_valid=True)\n",
    "    try:\n",
    "        assert len(valid_large_unseen_100) == 9* len(combined_cids) and len(valid_medium_unseen_100) == 7* len(combined_cids) and len(valid_small_unseen_100) == 5* len(combined_cids)\n",
    "    except AssertionError:\n",
    "        set_trace()\n",
    "        \n",
    "        \n",
    "    print('Validation set built')\n",
    "        \n",
    "    train_small, train_medium, train_large, train_ccs = build_train(corpus_dbscan[corpus_dbscan['id'].isin(train_ids)], train_ids, ccs)\n",
    "    try:\n",
    "        assert len(train_medium) == 12* len(combined_cids) and len(train_small) == 5* len(combined_cids)\n",
    "    except AssertionError:\n",
    "        set_trace()\n",
    "\n",
    "    print('Train set built')\n",
    "    \n",
    "    ccs = [train_ccs, (valid_ccs, valid_ccs_unseen_50, valid_ccs_unseen_100), (test_ccs, test_ccs_50, test_ccs_100)]\n",
    "    \n",
    "    print(f'Counted {counting} Cluster larger than 15')\n",
    "        \n",
    "    return (train_small, train_medium, train_large), ((valid_small, valid_medium, valid_large),(valid_small_unseen_50, valid_medium_unseen_50, valid_large_unseen_50),(valid_small_unseen_100, valid_medium_unseen_100, valid_large_unseen_100)), (test_set, test_set_unseen_50, test_set_unseen_100), ccs\n",
    "\n",
    "# function to build the three development set sizes from the train or validation split\n",
    "def build_train(corpus, ids, ccs, is_valid=False):\n",
    "    \n",
    "    corpus = corpus.copy()\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces]\n",
    "    cids = set(corpus['cluster_id'])\n",
    "    \n",
    "    corpus['title_processed'] = corpus['title'].apply(lower_to_unicode).copy()\n",
    "    corpus['title_processed'] = corpus['title_processed'].apply(preprocess_string, args=(CUSTOM_FILTERS,))\n",
    "    \n",
    "    all_pos = set()\n",
    "    all_neg = set()\n",
    "    \n",
    "    all_pos_small = set()\n",
    "    all_neg_small = set()\n",
    "    \n",
    "    all_pos_medium = set()\n",
    "    all_neg_medium = set()\n",
    "    \n",
    "    small = set()\n",
    "    medium = set()\n",
    "    large = set()\n",
    "    \n",
    "    small_ccs = set()\n",
    "    medium_ccs = set()\n",
    "    large_ccs = set()\n",
    "    \n",
    "    similarities=[sm.SoftTfIdf(corpus_list=corpus['title_processed'].tolist(), threshold=0.7),\n",
    "                          sm.Cosine(), \n",
    "                         sm.GeneralizedJaccard(threshold=0.7),\n",
    "                         sm.Dice(),\n",
    "                         FASTTEXT_MODEL]\n",
    "    budget_cc = 0\n",
    "    budgetrnd = 0\n",
    "    \n",
    "    for cid in cids:\n",
    "        \n",
    "        cur_small = set()\n",
    "        cur_medium = set()\n",
    "        cur_large = set()\n",
    "        \n",
    "        sub_corpus = corpus[corpus['cluster_id'] == cid]\n",
    "        sub_corpus_wo = corpus[~(corpus['cluster_id'] == cid)]\n",
    "        \n",
    "        if not is_valid:\n",
    "            group = sub_corpus.copy()\n",
    "            group_ids = group['id']\n",
    "            combs = list(combinations(group_ids, 2))\n",
    "            sims = []\n",
    "            for simno, sim in enumerate(similarities):\n",
    "                cur_res = []\n",
    "                for combination in combs:\n",
    "                    try:\n",
    "                        result = sim.get_sim_score(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                    except AttributeError:\n",
    "                        try:\n",
    "                            result = sim.get_raw_score(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                        except AttributeError:\n",
    "                            result = sim.n_similarity(group[group['id'] == combination[0]].iloc[0]['title_processed'], group[group['id'] == combination[1]].iloc[0]['title_processed'])\n",
    "                    cur_res.append(result)\n",
    "                sims.append(cur_res)\n",
    "            tupled = set()\n",
    "            for sim in sims:\n",
    "                tupled.update([(c, sim[i]) for i, c in enumerate(combs)])\n",
    "\n",
    "            sorted_sim = sorted(list(tupled), key = lambda y: y[1])\n",
    "            sorted_sim = [x[0] for x in sorted_sim]\n",
    "            check_cc_set = set()\n",
    "            for cur_set in ccs:\n",
    "                check_cc_set.update(cur_set)\n",
    "            is_cc = cid in check_cc_set\n",
    "\n",
    "            if is_cc:\n",
    "\n",
    "                cc_bucket = sorted_sim[:(math.ceil(len(sorted_sim)/5))]\n",
    "                rest_bucket = sorted_sim[(math.ceil(len(sorted_sim)/5)):]\n",
    "\n",
    "                first = set(*random.sample(cc_bucket, 1))\n",
    "                cur_small.update(first)\n",
    "                cur_medium.update(first)\n",
    "                cur_large.update(first)\n",
    "                small.update(first)\n",
    "                medium.update(first)\n",
    "                large.update(first)\n",
    "\n",
    "            else:\n",
    "                first = set(random.sample(group_ids.tolist(), 2))\n",
    "                cur_small.update(first)\n",
    "                cur_medium.update(first)\n",
    "                cur_large.update(first)\n",
    "                small.update(first)\n",
    "                medium.update(first)\n",
    "                large.update(first)\n",
    "        \n",
    "        for idx, (i, row) in enumerate(sub_corpus.sample(frac=1.0).iterrows()):\n",
    "            if is_valid:\n",
    "                if idx < 2:\n",
    "                    small.add(i)\n",
    "                if idx < 2:\n",
    "                    medium.add(i)\n",
    "                if idx < 2:\n",
    "                    large.add(i)\n",
    "            else:\n",
    "#                 if idx < 2:\n",
    "#                     small.add(i)\n",
    "                if len(cur_medium) < 3:\n",
    "                    cur_medium.add(i)\n",
    "                    medium.add(i)\n",
    "                if len(cur_large) < 11:\n",
    "                    cur_large.add(i)\n",
    "                    large.add(i)\n",
    "    \n",
    "        if not is_valid:\n",
    "            assert len(cur_small) == 2\n",
    "            assert len(cur_medium) == 3\n",
    "            assert len(cur_large) >= 3\n",
    "        \n",
    "    for cid in tqdm(cids):\n",
    "        \n",
    "        sub_corpus = corpus[corpus['cluster_id'] == cid]\n",
    "        sub_corpus_wo = corpus[~(corpus['cluster_id'] == cid)]\n",
    "        \n",
    "        \n",
    "        for num, size in enumerate([small, medium, large]):\n",
    "            \n",
    "            corpus_current = sub_corpus[sub_corpus['id'].isin(size)]\n",
    "            corpus_current_wo = sub_corpus_wo[sub_corpus_wo['id'].isin(size)]\n",
    "            \n",
    "            similarities=[sm.SoftTfIdf(corpus_list=corpus['title_processed'].tolist(), threshold=0.7),\n",
    "                          sm.Cosine(), \n",
    "                         sm.GeneralizedJaccard(threshold=0.7),\n",
    "                         sm.Dice(),\n",
    "                         FASTTEXT_MODEL]\n",
    "                              \n",
    "            \n",
    "            for i, row in corpus_current.iterrows():\n",
    "                \n",
    "                example = row['title_processed']\n",
    "                similarities_results = []\n",
    "                \n",
    "                \n",
    "                for sim in similarities:\n",
    "                    try:\n",
    "                        result = corpus_current_wo['title_processed'].apply(lambda x: sim.get_sim_score(example, x))\n",
    "                    except AttributeError:\n",
    "                        try:\n",
    "                            result = corpus_current_wo['title_processed'].apply(lambda x: sim.get_raw_score(example, x))\n",
    "                        except AttributeError:\n",
    "                            result = corpus_current_wo['title_processed'].apply(lambda x: sim.n_similarity(example, x))\n",
    "                    similarities_results.append(result)\n",
    "                sorted_sim = [x.sort_values(ascending=False) for x in similarities_results]\n",
    "\n",
    "                selected_negs = set()\n",
    "                already_selected_clusters = set()\n",
    "                ids_to_remove = set()\n",
    "                \n",
    "                if num == 0:\n",
    "                    limit = 1\n",
    "                elif num == 1:\n",
    "                    limit = 2\n",
    "                else:\n",
    "                    limit = 3\n",
    "                    \n",
    "                while len(selected_negs) < limit:\n",
    "\n",
    "                    index = random.sample(range(4), 1)[0]\n",
    "                    found = False\n",
    "                    counter = 0\n",
    "                    while not found:\n",
    "                        selected = sorted_sim[index].index[counter]\n",
    "                        rel_clu = sub_corpus_wo.loc[selected]['cluster_id']\n",
    "                        if num == 0:\n",
    "                            if rel_clu not in already_selected_clusters and (i,selected) not in all_neg_small and (selected,i) not in all_neg_small and (i,selected) not in selected_negs and (selected,i) not in selected_negs:\n",
    "                                selected_negs.update(random.sample([(i,selected), (selected,i)],1))\n",
    "                                ids_to_remove.add(selected)\n",
    "                                already_selected_clusters.add(rel_clu)\n",
    "                                found = True\n",
    "                        elif num == 1:\n",
    "                            if rel_clu not in already_selected_clusters and (i,selected) not in all_neg_medium and (selected,i) not in all_neg_medium and (i,selected) not in selected_negs and (selected,i) not in selected_negs:\n",
    "                                selected_negs.update(random.sample([(i,selected), (selected,i)],1))\n",
    "                                ids_to_remove.add(selected)\n",
    "                                already_selected_clusters.add(rel_clu)\n",
    "                                found = True\n",
    "                        else:\n",
    "                            if rel_clu not in already_selected_clusters and (i,selected) not in all_neg and (selected,i) not in all_neg and (i,selected) not in selected_negs and (selected,i) not in selected_negs:\n",
    "                                selected_negs.update(random.sample([(i,selected), (selected,i)],1))\n",
    "                                ids_to_remove.add(selected)\n",
    "                                already_selected_clusters.add(rel_clu)\n",
    "                                found = True\n",
    "                        counter += 1\n",
    "                            \n",
    "                if num == 0:\n",
    "                    all_neg_small.update(selected_negs)\n",
    "                    small_ccs.update(selected_negs)\n",
    "                    cur_length = len(all_neg_small)\n",
    "                    cur_selection = all_neg_small\n",
    "                elif num == 1:\n",
    "                    all_neg_medium.update(selected_negs)\n",
    "                    medium_ccs.update(selected_negs)\n",
    "                    cur_length = len(all_neg_medium)\n",
    "                    cur_selection = all_neg_medium\n",
    "                elif num == 2:\n",
    "                    all_neg.update(selected_negs)\n",
    "                    large_ccs.update(selected_negs)\n",
    "                    cur_length = len(all_neg)\n",
    "                    cur_selection = all_neg\n",
    "\n",
    "                rnd_sample_corpus = corpus_current_wo.loc[~corpus_current_wo.index.isin(ids_to_remove)]\n",
    "                \n",
    "                while len(cur_selection) == cur_length:\n",
    "                    rnd_id = rnd_sample_corpus.sample(1)['id'].iloc[0]\n",
    "                    rnd_pair = random.sample([(i, rnd_id), (rnd_id, i)], 1)\n",
    "                    if (i, rnd_id) not in cur_selection and (rnd_id, i) not in cur_selection:\n",
    "                        cur_selection.update(rnd_pair)\n",
    "                        \n",
    "                if num == 0:\n",
    "                    all_neg_small.update(cur_selection)\n",
    "                elif num == 1:\n",
    "                    all_neg_medium.update(cur_selection)\n",
    "                elif num == 2:\n",
    "                    all_neg.update(cur_selection)\n",
    "                    \n",
    "        positives = list(combinations(sub_corpus['id'].tolist(), 2))\n",
    "        positives_shuffled = [random.sample([(x[0],x[1]),(x[1], x[0])],1)[0] for x in positives]\n",
    "        all_pos.update(positives_shuffled)\n",
    "                \n",
    "        positives_small = list(combinations(small, 2))\n",
    "        positives_selected_small = [x for x in positives_shuffled if (x[0],x[1]) in positives_small or (x[1],x[0]) in positives_small]\n",
    "        all_pos_small.update(positives_selected_small)\n",
    "        \n",
    "        positives_medium = list(combinations(medium, 2))\n",
    "        positives_selected_medium = [x for x in positives_shuffled if (x[0],x[1]) in positives_medium or (x[1],x[0]) in positives_medium]\n",
    "        all_pos_medium.update(positives_selected_medium)\n",
    "        \n",
    "    large = all_pos | all_neg\n",
    "    medium = all_pos_medium | all_neg_medium\n",
    "    small = all_pos_small | all_neg_small\n",
    "    \n",
    "    return small, medium, large, (small_ccs, medium_ccs, large_ccs)\n",
    "\n",
    "# function to build the test split from the test offers\n",
    "def build_test(corpus, ids):\n",
    "    \n",
    "    corpus = corpus.copy()\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces]\n",
    "    cids = set(corpus['cluster_id'])\n",
    "    \n",
    "    corpus['title_processed'] = corpus['title'].apply(lower_to_unicode).copy()\n",
    "    corpus['title_processed'] = corpus['title_processed'].apply(preprocess_string, args=(CUSTOM_FILTERS,))\n",
    "    \n",
    "    all_pos = set()\n",
    "    all_neg = set()\n",
    "    \n",
    "    test_ccs = set()\n",
    "    \n",
    "    for cid in tqdm(cids):\n",
    "        sub_corpus = corpus[corpus['cluster_id'] == cid]\n",
    "        sub_corpus_wo = corpus[~(corpus['cluster_id'] == cid)]\n",
    "        \n",
    "        similarities=[sm.SoftTfIdf(corpus_list=corpus['title_processed'].tolist(), threshold=0.7),\n",
    "                     sm.Cosine(), \n",
    "                         sm.GeneralizedJaccard(threshold=0.7),\n",
    "                         sm.Dice(),\n",
    "                         FASTTEXT_MODEL]\n",
    "        \n",
    "        positives = list(combinations(sub_corpus['id'].tolist(), 2))\n",
    "        positives_shuffled = [random.sample([(x[0],x[1]),(x[1], x[0])],1)[0] for x in positives]\n",
    "\n",
    "        all_pos.update(positives_shuffled)\n",
    "        \n",
    "        for i, row in sub_corpus.iterrows():\n",
    "            \n",
    "            example = row['title_processed']\n",
    "            similarities_results = []\n",
    "            for sim in similarities:\n",
    "                try:\n",
    "                    result = sub_corpus_wo['title_processed'].apply(lambda x: sim.get_sim_score(example, x))\n",
    "                except AttributeError:\n",
    "                    try:\n",
    "                        result = sub_corpus_wo['title_processed'].apply(lambda x: sim.get_raw_score(example, x))\n",
    "                    except AttributeError:\n",
    "                        result = sub_corpus_wo['title_processed'].apply(lambda x: sim.n_similarity(example, x))\n",
    "                similarities_results.append(result)\n",
    "            sorted_sim = [x.sort_values(ascending=False) for x in similarities_results]\n",
    "            \n",
    "            selected_negs = set()\n",
    "            already_selected_clusters = set()\n",
    "            ids_to_remove = set()\n",
    "            \n",
    "            while len(selected_negs) < 3:\n",
    "                \n",
    "                index = random.sample(range(4), 1)[0]\n",
    "                found = False\n",
    "                counter = 0\n",
    "                while not found:\n",
    "                    selected = sorted_sim[index].index[counter]\n",
    "                    rel_clu = sub_corpus_wo.loc[selected]['cluster_id']\n",
    "                    if rel_clu not in already_selected_clusters and (i,selected) not in all_neg and (selected,i) not in all_neg and (i,selected) not in selected_negs and (selected,i) not in selected_negs:\n",
    "                        selected_negs.update(random.sample([(i,selected), (selected,i)],1))\n",
    "                        ids_to_remove.add(selected)\n",
    "                        already_selected_clusters.add(rel_clu)\n",
    "                        found = True\n",
    "                    counter += 1\n",
    "                \n",
    "            \n",
    "            test_ccs.update(selected_negs)\n",
    "            all_neg.update(selected_negs)\n",
    "            cur_length = len(all_neg)\n",
    "            rnd_sample_corpus = sub_corpus_wo.loc[~sub_corpus_wo.index.isin(ids_to_remove)]\n",
    "            \n",
    "            while len(all_neg) == cur_length:\n",
    "                rnd_id = rnd_sample_corpus.sample(1)['id'].iloc[0]\n",
    "\n",
    "                if (i, rnd_id) not in all_neg and (rnd_id, i) not in all_neg:\n",
    "                    all_neg.update(random.sample([(i, rnd_id), (rnd_id, i)], 1))\n",
    "            \n",
    "    all_ids = all_pos | all_neg\n",
    "    \n",
    "    return all_ids, test_ccs\n",
    "\n",
    "def check_if_hard(row, ccs):\n",
    "    left = row['id_left']\n",
    "    right = row['id_right']\n",
    "    if row['label'] == 1:\n",
    "        return False\n",
    "    for cc_set in ccs:\n",
    "        if left in cc_set and right in cc_set:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def generate_pairs(id_pairs, corpus, ccs):\n",
    "    id_pairs = list(id_pairs)\n",
    "    corpus = corpus[['id', 'brand', 'title', 'description', 'price', 'priceCurrency',\n",
    "       'specTableContent', 'cluster_id']]\n",
    "    left_ids, right_ids = list(zip(*id_pairs))\n",
    "\n",
    "    left_offers = corpus.loc[left_ids,:]\n",
    "    right_offers = corpus.loc[right_ids,:]\n",
    "    \n",
    "    left_offers = left_offers.reset_index(drop=True)\n",
    "    right_offers = right_offers.reset_index(drop=True)\n",
    "    \n",
    "    joined = left_offers.join(right_offers, lsuffix='_left', rsuffix='_right')\n",
    "    joined['pair_id'] = joined['id_left'].astype(str) + '#' + joined['id_right'].astype(str)\n",
    "    joined['label'] = joined['cluster_id_left'] == joined['cluster_id_right']\n",
    "    joined['label'] = joined['label'].astype(int)\n",
    "    joined['is_hard_negative'] = joined.apply(check_if_hard, args=(ccs,), axis=1)\n",
    "    return joined\n",
    "\n",
    "def generate_multiclass(pairwise_set, corpus, unseen):\n",
    "    ids = set()\n",
    "    pairwise_set = pairwise_set[pairwise_set['label'] == 1]\n",
    "    corpus = corpus[['id', 'brand', 'title', 'description', 'price', 'priceCurrency',\n",
    "       'specTableContent', 'cluster_id']]\n",
    "    ids.update(pairwise_set['id_left'])\n",
    "    ids.update(pairwise_set['id_right'])\n",
    "    ids = list(ids)\n",
    "    multiclass_set = corpus.loc[ids,:]\n",
    "    multiclass_set['label'] = multiclass_set['cluster_id']\n",
    "    multiclass_set['unseen'] = multiclass_set['cluster_id'].apply(lambda x: True if x in unseen else False)\n",
    "    multiclass_set = multiclass_set.reset_index(drop=True)\n",
    "    \n",
    "    return multiclass_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d30e48",
   "metadata": {},
   "source": [
    "# Load cleansed PDC2020 corpus and split into seen and unseen candidates given labeled DBSCAN clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b505e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_pickle('../../../data/interim/wdc-lspc/corpus/dedup_preprocessed_lspcV2020_only_en_strict_only_long_title_only_mainentity.pkl.gz')\n",
    "print(len(corpus))\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d82858b",
   "metadata": {},
   "source": [
    "# select hard negative clusters for all three difficulties, 80%, 50% and 20% as well as complentary random clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces]\n",
    "\n",
    "cc_candidates_seen = pd.read_csv('../../../data/interim/wdc-lspc/corpus/seen_dbscan_clusters_annotated.csv')\n",
    "cc_candidates_seen = cc_candidates_seen[cc_candidates_seen['good'] == 1]\n",
    "cc_candidates_seen = sorted(list(set(cc_candidates_seen['dbscan_cluster'])))\n",
    "\n",
    "cc_candidates_unseen = pd.read_csv('../../../data/interim/wdc-lspc/corpus/unseen_dbscan_clusters_annotated.csv')\n",
    "cc_candidates_unseen = cc_candidates_unseen[cc_candidates_unseen['good'] == 1]\n",
    "cc_candidates_unseen = sorted(list(set(cc_candidates_unseen['dbscan_cluster'])))\n",
    "\n",
    "dbscan_mapping = pd.read_csv('../../../data/interim/wdc-lspc/corpus/seen_dbscan_mapping.csv')\n",
    "corpus_dbscan = corpus.merge(dbscan_mapping, how='outer', on='cluster_id').copy()\n",
    "\n",
    "dbscan_mapping_unseen = pd.read_csv('../../../data/interim/wdc-lspc/corpus/unseen_dbscan_mapping.csv')\n",
    "corpus_dbscan_unseen = corpus.merge(dbscan_mapping_unseen, how='outer', on='cluster_id').copy()\n",
    "\n",
    "corpus_dbscan['title_processed'] = corpus_dbscan['title'].apply(lower_to_unicode)\n",
    "corpus_dbscan['title_processed'] = corpus_dbscan['title_processed'].apply(preprocess_string, args=(CUSTOM_FILTERS,))\n",
    "corpus_dbscan['title_processed'] = corpus_dbscan['title_processed'].apply(lambda x: ' '.join(x))\n",
    "corpus_dbscan = corpus_dbscan.drop_duplicates(subset='title_processed')\n",
    "\n",
    "corpus_dbscan_unseen['title_processed'] = corpus_dbscan_unseen['title'].apply(lower_to_unicode)\n",
    "corpus_dbscan_unseen['title_processed'] = corpus_dbscan_unseen['title_processed'].apply(preprocess_string, args=(CUSTOM_FILTERS,))\n",
    "corpus_dbscan_unseen['title_processed'] = corpus_dbscan_unseen['title_processed'].apply(lambda x: ' '.join(x))\n",
    "corpus_dbscan_unseen = corpus_dbscan_unseen.drop_duplicates(subset='title_processed')\n",
    "\n",
    "counts = corpus_dbscan['cluster_id'].value_counts()\n",
    "counts = counts[counts > 6]\n",
    "corpus_dbscan = corpus_dbscan[corpus_dbscan['cluster_id'].isin(counts.index)]\n",
    "\n",
    "counts_unseen = corpus_dbscan_unseen['cluster_id'].value_counts()\n",
    "counts_unseen = counts_unseen[counts_unseen > 3]\n",
    "counts_unseen = counts_unseen[counts_unseen < 7]\n",
    "corpus_dbscan_unseen = corpus_dbscan_unseen[corpus_dbscan_unseen['cluster_id'].isin(counts_unseen.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8107697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ccs_large, ccs_medium, ccs_small = select_cc_clusters(corpus_dbscan, cc_candidates_seen)\n",
    "ccs_large_unseen, ccs_medium_unseen, ccs_small_unseen = select_cc_clusters(corpus_dbscan_unseen, cc_candidates_unseen)\n",
    "\n",
    "ccs_unpack_unseen_large = set()\n",
    "ccs_unpack_unseen_large.update(*ccs_large_unseen)\n",
    "\n",
    "rnd_large, rnd_medium, rnd_small = select_rnd_clusters(corpus_dbscan, ccs_large, cc_candidates_seen)\n",
    "rnd_large_unseen, rnd_medium_unseen, rnd_small_unseen = select_rnd_clusters(corpus_dbscan_unseen, ccs_large_unseen, cc_candidates_unseen)\n",
    "\n",
    "merged = ccs_unpack_unseen_large | rnd_large_unseen\n",
    "\n",
    "corpus_dbscan_val = corpus_dbscan_unseen[~corpus_dbscan_unseen['cluster_id'].isin(merged)]\n",
    "\n",
    "ccs_large_val, ccs_medium_val, ccs_small_val = select_cc_clusters(corpus_dbscan_val, cc_candidates_unseen)\n",
    "rnd_large_val, rnd_medium_val, rnd_small_val = select_rnd_clusters(corpus_dbscan_val, ccs_large_val, cc_candidates_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3896c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure larger sets contain all clusters from smaller sets\n",
    "assert len(ccs_small & ccs_medium) == 20\n",
    "assert len(ccs_small & ccs_large) == 20\n",
    "assert len(ccs_medium & ccs_large) == 50\n",
    "\n",
    "assert len(rnd_small & rnd_medium) == 100\n",
    "assert len(rnd_small & rnd_large) == 100\n",
    "assert len(rnd_medium & rnd_large) == 250\n",
    "\n",
    "assert len(ccs_small_unseen & ccs_medium_unseen) == 20\n",
    "assert len(ccs_small_unseen & ccs_large_unseen) == 20\n",
    "assert len(ccs_medium_unseen & ccs_large_unseen) == 50\n",
    "\n",
    "assert len(rnd_small_unseen & rnd_medium_unseen) == 100\n",
    "assert len(rnd_small_unseen & rnd_large_unseen) == 100\n",
    "assert len(rnd_medium_unseen & rnd_large_unseen) == 250\n",
    "\n",
    "assert len(ccs_small_val & ccs_medium_val) == 20\n",
    "assert len(ccs_small_val & ccs_large_val) == 20\n",
    "assert len(ccs_medium_val & ccs_large_val) == 50\n",
    "\n",
    "assert len(rnd_small_val & rnd_medium_val) == 100\n",
    "assert len(rnd_small_val & rnd_large_val) == 100\n",
    "assert len(rnd_medium_val & rnd_large_val) == 250\n",
    "\n",
    "ccs_unpack_large = set()\n",
    "ccs_unpack_large.update(*ccs_large)\n",
    "\n",
    "ccs_unpack_medium = set()\n",
    "ccs_unpack_medium.update(*ccs_medium)\n",
    "\n",
    "ccs_unpack_small = set()\n",
    "ccs_unpack_small.update(*ccs_small)\n",
    "\n",
    "ccs_unpack_unseen_large = set()\n",
    "ccs_unpack_unseen_large.update(*ccs_large_unseen)\n",
    "\n",
    "ccs_unpack_unseen_medium = set()\n",
    "ccs_unpack_unseen_medium.update(*ccs_medium_unseen)\n",
    "\n",
    "ccs_unpack_unseen_small = set()\n",
    "ccs_unpack_unseen_small.update(*ccs_small_unseen)\n",
    "\n",
    "ccs_unpack_val_large = set()\n",
    "ccs_unpack_val_large.update(*ccs_large_val)\n",
    "\n",
    "ccs_unpack_val_medium = set()\n",
    "ccs_unpack_val_medium.update(*ccs_medium_val)\n",
    "\n",
    "ccs_unpack_val_small = set()\n",
    "ccs_unpack_val_small.update(*ccs_small_val)\n",
    "\n",
    "assert len((ccs_unpack_small | ccs_unpack_medium | ccs_unpack_large | rnd_small | rnd_medium | rnd_large) & (ccs_unpack_unseen_small | ccs_unpack_unseen_medium | ccs_unpack_unseen_large | rnd_small_unseen | rnd_medium_unseen | rnd_large_unseen)) == 0\n",
    "assert len((ccs_unpack_small | ccs_unpack_medium | ccs_unpack_large | rnd_small | rnd_medium | rnd_large) & (ccs_unpack_val_small | ccs_unpack_val_medium | ccs_unpack_val_large | rnd_small_val | rnd_medium_val | rnd_large_val)) == 0\n",
    "assert len((ccs_unpack_unseen_small | ccs_unpack_unseen_medium | ccs_unpack_unseen_large | rnd_small_unseen | rnd_medium_unseen | rnd_large_unseen) & (ccs_unpack_val_small | ccs_unpack_val_medium | ccs_unpack_val_large | rnd_small_val | rnd_medium_val | rnd_large_val)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce8733",
   "metadata": {},
   "source": [
    "# Generate training, validation and test sets for all hardness levels, development sizes and unseen percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce739744",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_80, valid_80, test_80, ccs_80 = build_pairs(pd.concat([corpus_dbscan, corpus_dbscan_unseen]), ccs_large, rnd_small, ccs_large_unseen, rnd_small_unseen, ccs_large_val, rnd_small_val)\n",
    "hard = ('80cc20rnd000un',[train_80, valid_80, test_80, ccs_80])\n",
    "\n",
    "train_50, valid_50, test_50, ccs_50 = build_pairs(pd.concat([corpus_dbscan, corpus_dbscan_unseen]), ccs_medium, rnd_medium, ccs_medium_unseen, rnd_medium_unseen, ccs_medium_val, rnd_medium_val)\n",
    "medium = ('50cc50rnd000un',[train_50, valid_50, test_50, ccs_50])\n",
    "\n",
    "train_20, valid_20, test_20, ccs_20 = build_pairs(pd.concat([corpus_dbscan, corpus_dbscan_unseen]), ccs_small, rnd_large, ccs_small_unseen, rnd_large_unseen, ccs_small_val, rnd_large_val)\n",
    "easy = ('20cc80rnd000un',[train_20, valid_20, test_20, ccs_20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0fdae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen = ccs_unpack_unseen_large | ccs_unpack_val_large | rnd_large_unseen |rnd_large_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcea7bea",
   "metadata": {},
   "source": [
    "# Materialize datasets and write them to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['training-sets', 'validation-sets', 'gold-standards']\n",
    "\n",
    "for cur_path in paths:\n",
    "    path = pathlib.Path(f'../../../data/raw/wdc-lspc/{cur_path}/')\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for combination in [hard, medium, easy]:\n",
    "    name = combination[0]\n",
    "    train, valid, test, ccs = combination[1]\n",
    "    \n",
    "    valid_050_unseen = valid[1]\n",
    "    valid_100_unseen = valid[2]\n",
    "    valid = valid[0]\n",
    "    \n",
    "    train_ccs = ccs[0]\n",
    "    valid_ccs = ccs[1][0]\n",
    "    valid_ccs_050_unseen = ccs[1][1]\n",
    "    valid_ccs_100_unseen = ccs[1][2]\n",
    "    test_ccs = ccs[2]\n",
    "    \n",
    "    test_000_unseen = generate_pairs(test[0], corpus, test_ccs[0]).sample(frac=1.0, random_state=42)\n",
    "    test_050_unseen = generate_pairs(test[1], corpus, test_ccs[1]).sample(frac=1.0, random_state=42)\n",
    "    test_100_unseen = generate_pairs(test[2], corpus, test_ccs[2]).sample(frac=1.0, random_state=42)\n",
    "\n",
    "    test_multi_000_unseen = generate_multiclass(test_000_unseen, corpus, unseen).sample(frac=1.0, random_state=42)\n",
    "    test_multi_050_unseen = generate_multiclass(test_050_unseen, corpus, unseen).sample(frac=1.0, random_state=42)\n",
    "    test_multi_100_unseen = generate_multiclass(test_100_unseen, corpus, unseen).sample(frac=1.0, random_state=42)\n",
    "\n",
    "    train_small = generate_pairs(train[0], corpus, train_ccs[0]).sample(frac=1.0, random_state=42)\n",
    "    train_medium = generate_pairs(train[1], corpus, train_ccs[1]).sample(frac=1.0, random_state=42)\n",
    "    train_large = generate_pairs(train[2], corpus, train_ccs[2]).sample(frac=1.0, random_state=42)\n",
    "\n",
    "    train_small_multi = generate_multiclass(train_small, corpus, unseen).sample(frac=1.0, random_state=42)\n",
    "    train_medium_multi = generate_multiclass(train_medium, corpus, unseen).sample(frac=1.0, random_state=42)\n",
    "    train_large_multi = generate_multiclass(train_large, corpus, unseen).sample(frac=1.0, random_state=42)\n",
    "\n",
    "    valid_small = generate_pairs(valid[0], corpus, valid_ccs[0]).sample(frac=1.0, random_state=42)\n",
    "    valid_medium = generate_pairs(valid[1], corpus, valid_ccs[1]).sample(frac=1.0, random_state=42)\n",
    "    valid_large = generate_pairs(valid[2], corpus, valid_ccs[2]).sample(frac=1.0, random_state=42)\n",
    "\n",
    "    valid_small_multi = generate_multiclass(valid_small, corpus, unseen).sample(frac=1.0, random_state=42)\n",
    "    valid_medium_multi = generate_multiclass(valid_medium, corpus, unseen).sample(frac=1.0, random_state=42)\n",
    "    valid_large_multi = generate_multiclass(valid_large, corpus, unseen).sample(frac=1.0, random_state=42)\n",
    "    \n",
    "    valid_small_050_unseen = generate_pairs(valid_050_unseen[0], corpus, valid_ccs_050_unseen[0]).sample(frac=1.0, random_state=42)\n",
    "    valid_medium_050_unseen = generate_pairs(valid_050_unseen[1], corpus, valid_ccs_050_unseen[1]).sample(frac=1.0, random_state=42)\n",
    "    valid_large_050_unseen = generate_pairs(valid_050_unseen[2], corpus, valid_ccs_050_unseen[2]).sample(frac=1.0, random_state=42)\n",
    "\n",
    "    valid_small_multi_050_unseen = generate_multiclass(valid_small_050_unseen, corpus, unseen).sample(frac=1.0, random_state=42)\n",
    "    valid_medium_multi_050_unseen = generate_multiclass(valid_medium_050_unseen, corpus, unseen).sample(frac=1.0, random_state=42)\n",
    "    valid_large_multi_050_unseen = generate_multiclass(valid_large_050_unseen, corpus, unseen).sample(frac=1.0, random_state=42)\n",
    "    \n",
    "    valid_small_100_unseen = generate_pairs(valid_050_unseen[0], corpus, valid_ccs_050_unseen[0]).sample(frac=1.0, random_state=42)\n",
    "    valid_medium_100_unseen = generate_pairs(valid_050_unseen[1], corpus, valid_ccs_050_unseen[1]).sample(frac=1.0, random_state=42)\n",
    "    valid_large_100_unseen = generate_pairs(valid_050_unseen[2], corpus, valid_ccs_050_unseen[2]).sample(frac=1.0, random_state=42)\n",
    "\n",
    "    valid_small_multi_100_unseen = generate_multiclass(valid_small_100_unseen, corpus, unseen).sample(frac=1.0, random_state=42)\n",
    "    valid_medium_multi_100_unseen = generate_multiclass(valid_medium_100_unseen, corpus, unseen).sample(frac=1.0, random_state=42)\n",
    "    valid_large_multi_100_unseen = generate_multiclass(valid_large_100_unseen, corpus, unseen).sample(frac=1.0, random_state=42)\n",
    "    \n",
    "    train_small.to_json(f'../../../data/raw/wdc-lspc/training-sets/wdcproducts{name}_train_small.json.gz', lines=True, orient='records')\n",
    "    train_medium.to_json(f'../../../data/raw/wdc-lspc/training-sets/wdcproducts{name}_train_medium.json.gz', lines=True, orient='records')\n",
    "    train_large.to_json(f'../../../data/raw/wdc-lspc/training-sets/wdcproducts{name}_train_large.json.gz', lines=True, orient='records')\n",
    "    \n",
    "    train_small_multi.to_json(f'../../../data/raw/wdc-lspc/training-sets/wdcproductsmulti{name}_train_small.json.gz', lines=True, orient='records')\n",
    "    train_medium_multi.to_json(f'../../../data/raw/wdc-lspc/training-sets/wdcproductsmulti{name}_train_medium.json.gz', lines=True, orient='records')\n",
    "    train_large_multi.to_json(f'../../../data/raw/wdc-lspc/training-sets/wdcproductsmulti{name}_train_large.json.gz', lines=True, orient='records')\n",
    "\n",
    "    valid_small.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproducts{name}_valid_small.json.gz', lines=True, orient='records')\n",
    "    valid_medium.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproducts{name}_valid_medium.json.gz', lines=True, orient='records')\n",
    "    valid_large.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproducts{name}_valid_large.json.gz', lines=True, orient='records')\n",
    "    \n",
    "    valid_small_multi.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproductsmulti{name}_valid_small.json.gz', lines=True, orient='records')\n",
    "    valid_medium_multi.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproductsmulti{name}_valid_medium.json.gz', lines=True, orient='records')\n",
    "    valid_large_multi.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproductsmulti{name}_valid_large.json.gz', lines=True, orient='records')\n",
    "    \n",
    "    valid_small_050_unseen.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproducts{name.replace(\"000un\", \"050un\")}_valid_small.json.gz', lines=True, orient='records')\n",
    "    valid_medium_050_unseen.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproducts{name.replace(\"000un\", \"050un\")}_valid_medium.json.gz', lines=True, orient='records')\n",
    "    valid_large_050_unseen.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproducts{name.replace(\"000un\", \"050un\")}_valid_large.json.gz', lines=True, orient='records')\n",
    "    \n",
    "    valid_small_100_unseen.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproducts{name.replace(\"000un\", \"100un\")}_valid_small.json.gz', lines=True, orient='records')\n",
    "    valid_medium_100_unseen.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproducts{name.replace(\"000un\", \"100un\")}_valid_medium.json.gz', lines=True, orient='records')\n",
    "    valid_large_100_unseen.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproducts{name.replace(\"000un\", \"100un\")}_valid_large.json.gz', lines=True, orient='records')\n",
    "    \n",
    "    valid_small_multi_050_unseen.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproductsmulti{name.replace(\"000un\", \"050un\")}_valid_small.json.gz', lines=True, orient='records')\n",
    "    valid_medium_multi_050_unseen.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproductsmulti{name.replace(\"000un\", \"050un\")}_valid_medium.json.gz', lines=True, orient='records')\n",
    "    valid_large_multi_050_unseen.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproductsmulti{name.replace(\"000un\", \"050un\")}_valid_large.json.gz', lines=True, orient='records')\n",
    "    \n",
    "    valid_small_multi_100_unseen.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproductsmulti{name.replace(\"000un\", \"100un\")}_valid_small.json.gz', lines=True, orient='records')\n",
    "    valid_medium_multi_100_unseen.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproductsmulti{name.replace(\"000un\", \"100un\")}_valid_medium.json.gz', lines=True, orient='records')\n",
    "    valid_large_multi_100_unseen.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproductsmulti{name.replace(\"000un\", \"100un\")}_valid_large.json.gz', lines=True, orient='records')\n",
    "       \n",
    "    test_000_unseen.to_json(f'../../../data/raw/wdc-lspc/gold-standards/wdcproducts{name}_gs.json.gz', lines=True, orient='records')\n",
    "    test_multi_000_unseen.to_json(f'../../../data/raw/wdc-lspc/gold-standards/wdcproductsmulti{name}_gs.json.gz', lines=True, orient='records')\n",
    "    \n",
    "    test_050_unseen.to_json(f'../../../data/raw/wdc-lspc/gold-standards/wdcproducts{name.replace(\"000un\", \"050un\")}_gs.json.gz', lines=True, orient='records')\n",
    "    test_multi_050_unseen.to_json(f'../../../data/raw/wdc-lspc/gold-standards/wdcproductsmulti{name.replace(\"000un\", \"050un\")}_gs.json.gz', lines=True, orient='records')\n",
    "    \n",
    "    test_100_unseen.to_json(f'../../../data/raw/wdc-lspc/gold-standards/wdcproducts{name.replace(\"000un\", \"100un\")}_gs.json.gz', lines=True, orient='records')\n",
    "    test_multi_100_unseen.to_json(f'../../../data/raw/wdc-lspc/gold-standards/wdcproductsmulti{name.replace(\"000un\", \"100un\")}_gs.json.gz', lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841de8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
