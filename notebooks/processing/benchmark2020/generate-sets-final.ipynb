{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62273701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from itertools import combinations\n",
    "import math\n",
    "\n",
    "from gensim.parsing.preprocessing import lower_to_unicode, preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric\n",
    "\n",
    "import py_stringmatching as sm\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493ee65",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that selects hard negatives in DBSCAN clusters using SoftTFIDF and Cosine similarity\n",
    "def select_cc_clusters(corpus_dbscan, cc_candidates_seen):\n",
    "    \n",
    "    corpus_dbscan = corpus_dbscan.set_index('cluster_id', drop=False).copy()\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces]\n",
    "    \n",
    "    selection_final = []\n",
    "    selection_final_tupled = []\n",
    "    amounts = [400, 250, 100]\n",
    "    \n",
    "    random.seed(42)\n",
    "    random.shuffle(cc_candidates_seen)\n",
    "    \n",
    "    for amount in amounts:\n",
    "        sub_corpus_dbscan = corpus_dbscan.copy()\n",
    "        cur_selection = set()\n",
    "        cur_selection_tupled = set()\n",
    "\n",
    "        while len(cur_selection) < amount:\n",
    "            for dbscan_id in cc_candidates_seen:\n",
    "                sub_corpus = sub_corpus_dbscan[sub_corpus_dbscan['dbscan_cluster'] == dbscan_id].drop_duplicates('cluster_id').copy()\n",
    "                if len(sub_corpus) < 5:\n",
    "                    continue\n",
    "\n",
    "                sub_corpus['title_processed'] = sub_corpus['title'].apply(lower_to_unicode)\n",
    "                sub_corpus['title_processed'] = sub_corpus['title_processed'].apply(preprocess_string, args=(CUSTOM_FILTERS,))\n",
    "\n",
    "                similarities=[sm.SoftTfIdf(corpus_list=sub_corpus['title_processed'].tolist()), sm.Cosine()]\n",
    "\n",
    "                sub_cids = sorted(list(set(sub_corpus['cluster_id'])))\n",
    "                random.seed(42)\n",
    "                random_pick = random.sample(sub_cids, 1)\n",
    "                similarities_results = []\n",
    "                sub_selection = set()\n",
    "\n",
    "                example = sub_corpus.loc[random_pick]['title_processed'].iloc[0]\n",
    "                sub_corpus = sub_corpus.drop(random_pick)\n",
    "\n",
    "                for sim in similarities:\n",
    "                    try:\n",
    "                        result = sub_corpus['title_processed'].apply(lambda x: sim.get_sim_score(example, x))\n",
    "                    except AttributeError:\n",
    "                        result = sub_corpus['title_processed'].apply(lambda x: sim.get_raw_score(example, x))\n",
    "                    similarities_results.append(result)\n",
    "                sorted_sim = [x.sort_values(ascending=False) for x in similarities_results]\n",
    "\n",
    "                for i  in range(len(sorted_sim[0])):\n",
    "                    sub_selection.add(sorted_sim[0].index[i])\n",
    "                    if len(sub_selection) > 3:\n",
    "                        break\n",
    "                    sub_selection.add(sorted_sim[1].index[i])\n",
    "                    if len(sub_selection) > 3:\n",
    "                        break\n",
    "\n",
    "                sub_selection.update(random_pick)\n",
    "                cur_selection.update(sub_selection)\n",
    "                cur_selection_tupled.add(tuple(sub_selection))\n",
    "\n",
    "                sub_corpus_dbscan = sub_corpus_dbscan.loc[~sub_corpus_dbscan.index.isin(cur_selection)]\n",
    "                if len(cur_selection) == amount:\n",
    "                    break\n",
    "\n",
    "        assert len(cur_selection) == amount\n",
    "        selection_final.append(cur_selection)\n",
    "        selection_final_tupled.append(cur_selection_tupled)\n",
    "        \n",
    "    return selection_final_tupled\n",
    "\n",
    "# function that selects random clusters\n",
    "def select_rnd_clusters(corpus_dbscan, ccs, cc_candidates_seen):\n",
    "\n",
    "    ccs_set = set()\n",
    "    ccs_set.update(*ccs)\n",
    "\n",
    "    corpus_dbscan = corpus_dbscan[~corpus_dbscan['cluster_id'].isin(ccs_set)].copy()\n",
    "    corpus_dbscan = corpus_dbscan[corpus_dbscan['dbscan_cluster'].isin(cc_candidates_seen)]\n",
    "    \n",
    "    selection_final = []\n",
    "    amounts = [400, 250, 100]\n",
    "    \n",
    "    sub_corpus = corpus_dbscan.set_index('cluster_id', drop=False).copy()\n",
    "\n",
    "    #counts = sub_corpus['cluster_id'].value_counts()\n",
    "    #counts = counts[counts >6]\n",
    "    #sub_corpus = sub_corpus[sub_corpus['cluster_id'].isin(counts.index)]\n",
    "\n",
    "    sub_corpus = sub_corpus.sort_values('id')\n",
    "    sub_corpus = sub_corpus.set_index('id', drop=False)\n",
    "    sub_corpus = sub_corpus.drop_duplicates('cluster_id')\n",
    "    rnd_cids = sorted(list(set(sub_corpus['cluster_id'])))\n",
    "\n",
    "    random.seed(42)\n",
    "    sample = random.sample(rnd_cids, amounts[0])\n",
    "    \n",
    "    for amount in amounts:\n",
    "        sub_sample = set(sample[:amount])\n",
    "        assert len(sub_sample) == amount\n",
    "        selection_final.append(sub_sample)\n",
    "    \n",
    "    return selection_final\n",
    "\n",
    "# funtion to build pairs from the created train, validation and test splits\n",
    "def build_pairs(corpus, ccs, rnd, ccs_unseen, rnd_unseen):\n",
    "    \n",
    "    # seen\n",
    "    ccs_set = set()\n",
    "    ccs_set.update(*ccs)\n",
    "    \n",
    "    combined_cids = list(ccs_set | rnd)\n",
    "    corpus_dbscan = corpus.sort_values('id').copy()\n",
    "    corpus_dbscan = corpus_dbscan.set_index('cluster_id', drop=False)\n",
    "    \n",
    "    corpus_dbscan = corpus_dbscan.loc[combined_cids]\n",
    "    corpus_dbscan = corpus_dbscan.set_index('id', drop=False)\n",
    "    \n",
    "    sample_to_10 = set()\n",
    "    test_ids = set()\n",
    "    valid_ids = set()\n",
    "    train_ids = set()\n",
    "    \n",
    "    for name, group in corpus_dbscan.groupby('cluster_id'):\n",
    "        \n",
    "        if len(group) > 7:\n",
    "            max_len = len(group)\n",
    "            if max_len < 15:\n",
    "                cur_10 = sorted(list(set(group.sample(max_len)['id'])))\n",
    "            else:\n",
    "                cur_10 = sorted(list(set(group.sample(15)['id'])))\n",
    "        else:\n",
    "            cur_10 = sorted(list(set(group.sample(7)['id'])))\n",
    "        sample_to_10.update(cur_10)\n",
    "        \n",
    "        random.seed(42)\n",
    "        test_ids.update(set(random.sample(cur_10, 2)))\n",
    "        remaining = sorted(list(set(cur_10) - test_ids))\n",
    "        random.seed(42)\n",
    "        valid_ids.update(set(random.sample(remaining, 2)))\n",
    "        remaining = set(remaining) - valid_ids\n",
    "        train_ids.update(remaining)\n",
    "        \n",
    "        \n",
    "    assert len(test_ids & valid_ids & train_ids) == 0\n",
    "\n",
    "    corpus_dbscan = corpus_dbscan[corpus_dbscan['id'].isin(sample_to_10)]\n",
    "\n",
    "    # unseen 50\n",
    "    random.seed(42)\n",
    "    ccs_seen_50 = random.sample(list(ccs), math.ceil(len(ccs)/2))\n",
    "    random.seed(42)\n",
    "    rnd_seen_50 = random.sample(list(rnd), math.ceil(len(rnd)/2))\n",
    "    random.seed(42)\n",
    "    ccs_unseen_50 = random.sample(list(ccs_unseen), math.ceil(len(ccs_unseen)/2))\n",
    "    random.seed(42)\n",
    "    rnd_unseen_50 = random.sample(list(rnd_unseen), math.ceil(len(rnd_unseen)/2))\n",
    "    \n",
    "    ccs_seen_50_set = set()\n",
    "    ccs_seen_50_set.update(*ccs_seen_50)\n",
    "    rnd_seen_50_set = set()\n",
    "    rnd_seen_50_set.update(rnd_seen_50)\n",
    "    ccs_unseen_50_set = set()\n",
    "    ccs_unseen_50_set.update(*ccs_unseen_50)\n",
    "    rnd_unseen_50_set = set()\n",
    "    rnd_unseen_50_set.update(rnd_unseen_50)\n",
    "    \n",
    "    combined_cids_unseen_50 = ccs_seen_50_set | rnd_seen_50_set | ccs_unseen_50_set | rnd_unseen_50_set\n",
    "    corpus_dbscan_unseen_50 = corpus.sort_values('id').copy()\n",
    "    corpus_dbscan_unseen_50 = corpus_dbscan_unseen_50.set_index('cluster_id', drop=False)\n",
    "    \n",
    "    corpus_dbscan_unseen_50 = corpus_dbscan_unseen_50.loc[combined_cids_unseen_50]\n",
    "    corpus_dbscan_unseen_50 = corpus_dbscan_unseen_50.set_index('id', drop=False)\n",
    "    \n",
    "    sample_to_3_unseen_50 = set()\n",
    "    test_ids_unseen_50 = set()\n",
    "    \n",
    "    for name, group in corpus_dbscan_unseen_50.groupby('cluster_id'):\n",
    "        \n",
    "        cur_3_unseen_50 = sorted(list(set(group.sample(2)['id'])))\n",
    "        sample_to_3_unseen_50.update(cur_3_unseen_50)\n",
    "        \n",
    "        random.seed(42)\n",
    "        test_ids_unseen_50.update(sample_to_3_unseen_50)\n",
    "\n",
    "    corpus_dbscan_unseen_50 = corpus_dbscan_unseen_50[corpus_dbscan_unseen_50['id'].isin(sample_to_3_unseen_50)]\n",
    "    \n",
    "     # unseen 100\n",
    "    ccs_unseen_set = set()\n",
    "    ccs_unseen_set.update(*ccs_unseen)\n",
    "    \n",
    "    combined_cids_unseen_100 = list(ccs_unseen_set | rnd_unseen)\n",
    "    corpus_dbscan_unseen_100 = corpus.sort_values('id').copy()\n",
    "    corpus_dbscan_unseen_100 = corpus_dbscan_unseen_100.set_index('cluster_id', drop=False)\n",
    "    \n",
    "    corpus_dbscan_unseen_100 = corpus_dbscan_unseen_100.loc[combined_cids_unseen_100]\n",
    "    corpus_dbscan_unseen_100 = corpus_dbscan_unseen_100.set_index('id', drop=False)\n",
    "    \n",
    "    sample_to_3_unseen_100 = set()\n",
    "    test_ids_unseen_100 = set()\n",
    "    \n",
    "    for name, group in corpus_dbscan_unseen_100.groupby('cluster_id'):\n",
    "        \n",
    "        cur_3_unseen_100 = sorted(list(set(group.sample(2)['id'])))\n",
    "        sample_to_3_unseen_100.update(cur_3_unseen_100)\n",
    "        \n",
    "        random.seed(42)\n",
    "        test_ids_unseen_100.update(sample_to_3_unseen_100)\n",
    "\n",
    "    corpus_dbscan_unseen_100 = corpus_dbscan_unseen_100[corpus_dbscan_unseen_100['id'].isin(sample_to_3_unseen_100)]\n",
    "    \n",
    "    # build test, valid, train\n",
    "    test_set = build_test(corpus_dbscan[corpus_dbscan['id'].isin(test_ids)], test_ids)\n",
    "    try:\n",
    "        assert len(test_set) == 9* len(combined_cids)\n",
    "    except AssertionError:\n",
    "        set_trace()\n",
    "\n",
    "    test_set_unseen_50 = build_test(corpus_dbscan_unseen_50[corpus_dbscan_unseen_50['id'].isin(test_ids_unseen_50)], test_ids_unseen_50)\n",
    "    try:\n",
    "        assert len(test_set_unseen_50) == 9* len(combined_cids)\n",
    "    except AssertionError:\n",
    "        set_trace()\n",
    "        \n",
    "    test_set_unseen_100 = build_test(corpus_dbscan_unseen_100[corpus_dbscan_unseen_100['id'].isin(test_ids_unseen_100)], test_ids_unseen_100)\n",
    "    try:\n",
    "        assert len(test_set_unseen_100) == 9* len(combined_cids)\n",
    "    except AssertionError:\n",
    "        set_trace()\n",
    "        \n",
    "    print('Test set built')\n",
    "    \n",
    "    valid_small, valid_medium, valid_large = build_train(corpus_dbscan[corpus_dbscan['id'].isin(valid_ids)], valid_ids)\n",
    "    try:\n",
    "        assert len(valid_large) == 9* len(combined_cids) and len(valid_medium) == 7* len(combined_cids) and len(valid_small) == 5* len(combined_cids)\n",
    "    except AssertionError:\n",
    "        set_trace()\n",
    "        \n",
    "    print('Validation set built')\n",
    "        \n",
    "    train_small, train_medium, train_large = build_train(corpus_dbscan[corpus_dbscan['id'].isin(train_ids)], train_ids)\n",
    "    try:\n",
    "        assert len(train_medium) == 12* len(combined_cids) and len(train_small) == 5* len(combined_cids)\n",
    "    except AssertionError:\n",
    "        set_trace()\n",
    "\n",
    "    print('Train set built')\n",
    "        \n",
    "    return (train_small, train_medium, train_large), (valid_small, valid_medium, valid_large), (test_set, test_set_unseen_50, test_set_unseen_100)\n",
    "\n",
    "# function to build the three development set sizes from the train or validation split\n",
    "def build_train(corpus, ids, is_valid=False):\n",
    "    \n",
    "    corpus = corpus.copy()\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces]\n",
    "    cids = set(corpus['cluster_id'])\n",
    "    \n",
    "    corpus['title_processed'] = corpus['title'].apply(lower_to_unicode).copy()\n",
    "    corpus['title_processed'] = corpus['title_processed'].apply(preprocess_string, args=(CUSTOM_FILTERS,))\n",
    "    \n",
    "    all_pos = set()\n",
    "    all_neg = set()\n",
    "    \n",
    "    all_pos_small = set()\n",
    "    all_neg_small = set()\n",
    "    \n",
    "    all_pos_medium = set()\n",
    "    all_neg_medium = set()\n",
    "    \n",
    "    small = set()\n",
    "    medium = set()\n",
    "    large = set()\n",
    "    \n",
    "    for cid in cids:\n",
    "        sub_corpus = corpus[corpus['cluster_id'] == cid]\n",
    "        sub_corpus_wo = corpus[~(corpus['cluster_id'] == cid)]\n",
    "        \n",
    "        for idx, (i, row) in enumerate(sub_corpus.iterrows()):\n",
    "            if is_valid:\n",
    "                if idx < 2:\n",
    "                    small.add(i)\n",
    "                if idx < 2:\n",
    "                    medium.add(i)\n",
    "                if idx < 2:\n",
    "                    large.add(i)\n",
    "            else:\n",
    "                if idx < 2:\n",
    "                    small.add(i)\n",
    "                if idx < 3:\n",
    "                    medium.add(i)\n",
    "                if idx < 11:\n",
    "                    large.add(i)\n",
    "\n",
    "    for cid in tqdm(cids):\n",
    "        \n",
    "        sub_corpus = corpus[corpus['cluster_id'] == cid]\n",
    "        sub_corpus_wo = corpus[~(corpus['cluster_id'] == cid)]\n",
    "        \n",
    "        \n",
    "        for num, size in enumerate([small, medium, large]):\n",
    "            \n",
    "            corpus_current = sub_corpus[sub_corpus['id'].isin(size)]\n",
    "            corpus_current_wo = sub_corpus_wo[sub_corpus_wo['id'].isin(size)]\n",
    "            \n",
    "            similarities=[sm.SoftTfIdf(corpus_list=corpus_current_wo['title_processed'].tolist()), sm.Cosine()]\n",
    "            \n",
    "            for i, row in corpus_current.iterrows():\n",
    "                \n",
    "                example = row['title_processed']\n",
    "                similarities_results = []\n",
    "                \n",
    "                \n",
    "                for sim in similarities:\n",
    "                    try:\n",
    "                        result = corpus_current_wo['title_processed'].apply(lambda x: sim.get_sim_score(example, x))\n",
    "                    except AttributeError:\n",
    "                        result = corpus_current_wo['title_processed'].apply(lambda x: sim.get_raw_score(example, x))\n",
    "                    similarities_results.append(result)\n",
    "                sorted_sim = [x.sort_values(ascending=False) for x in similarities_results]\n",
    "\n",
    "                selected_negs = set()\n",
    "                already_selected_clusters = set()\n",
    "                ids_to_remove = set()\n",
    "                \n",
    "                for run, similarity in enumerate(sorted_sim):\n",
    "                    \n",
    "                    for idx in similarity.index:\n",
    "                        \n",
    "                        # check if enough sampled\n",
    "                        if len(selected_negs) == 1 and num == 0:\n",
    "                            break\n",
    "                        if len(selected_negs) == 2 and num == 1:\n",
    "                            break\n",
    "                        if len(selected_negs) == 3 and num == 2:\n",
    "                            break\n",
    "                            \n",
    "                        # check small\n",
    "                        if len(selected_negs) == 1 and run == 0 and num == 0:\n",
    "                            break\n",
    "                            \n",
    "                        # check medium  \n",
    "                        if len(selected_negs) == 1 and run == 0 and num == 1:\n",
    "                            break\n",
    "                        elif len(selected_negs) == 2 and run == 1 and num == 1:\n",
    "                            break\n",
    "                            \n",
    "                        # check large\n",
    "                        if len(selected_negs) == 2 and run == 0 and num == 2:\n",
    "                            break\n",
    "                        elif len(selected_negs) == 3 and run == 1 and num == 2:\n",
    "                            break\n",
    "                            \n",
    "                        rel_clu = corpus_current_wo.loc[idx]['cluster_id']\n",
    "\n",
    "                        if num == 0:\n",
    "                            if rel_clu not in already_selected_clusters and (i,idx) not in all_neg_small and (idx,i) not in all_neg_small:\n",
    "                                random.seed(42)\n",
    "                                selected_negs.update(random.sample([(i,idx), (idx,i)],1))\n",
    "                                ids_to_remove.add(idx)\n",
    "                                already_selected_clusters.add(rel_clu)\n",
    "                        elif num == 1:\n",
    "                            if rel_clu not in already_selected_clusters and (i,idx) not in all_neg_medium and (idx,i) not in all_neg_medium:\n",
    "                                random.seed(42)\n",
    "                                selected_negs.update(random.sample([(i,idx), (idx,i)],1))\n",
    "                                ids_to_remove.add(idx)\n",
    "                                already_selected_clusters.add(rel_clu)\n",
    "                        elif num == 2:\n",
    "                            if rel_clu not in already_selected_clusters and (i,idx) not in all_neg and (idx,i) not in all_neg:\n",
    "                                random.seed(42)\n",
    "                                selected_negs.update(random.sample([(i,idx), (idx,i)],1))\n",
    "                                ids_to_remove.add(idx)\n",
    "                                already_selected_clusters.add(rel_clu)\n",
    "                            \n",
    "                if num == 0:\n",
    "                    all_neg_small.update(selected_negs)\n",
    "                    cur_length = len(all_neg_small)\n",
    "                    cur_selection = all_neg_small\n",
    "                elif num == 1:\n",
    "                    all_neg_medium.update(selected_negs)\n",
    "                    cur_length = len(all_neg_medium)\n",
    "                    cur_selection = all_neg_medium\n",
    "                elif num == 2:\n",
    "                    all_neg.update(selected_negs)\n",
    "                    cur_length = len(all_neg)\n",
    "                    cur_selection = all_neg\n",
    "\n",
    "                rnd_sample_corpus = corpus_current_wo.loc[~corpus_current_wo.index.isin(ids_to_remove)]\n",
    "\n",
    "                cur_seed = 42\n",
    "                \n",
    "                while len(cur_selection) == cur_length:\n",
    "                    rnd_id = rnd_sample_corpus.sample(1, random_state=cur_seed)['id'].iloc[0]\n",
    "                    random.seed(42)\n",
    "                    rnd_pair = random.sample([(i, rnd_id), (rnd_id, i)], 1)\n",
    "                    cur_seed += 1\n",
    "                    if (i, rnd_id) not in cur_selection and (rnd_id, i) not in cur_selection:\n",
    "                        cur_selection.update(rnd_pair)\n",
    "                        \n",
    "                if num == 0:\n",
    "                    all_neg_small.update(cur_selection)\n",
    "                elif num == 1:\n",
    "                    all_neg_medium.update(cur_selection)\n",
    "                elif num == 2:\n",
    "                    all_neg.update(cur_selection)\n",
    "                    \n",
    "        positives = list(combinations(sub_corpus['id'].tolist(), 2))\n",
    "        random.seed(42)\n",
    "        positives_shuffled = [random.sample([(x[0],x[1]),(x[1], x[0])],1)[0] for x in positives]\n",
    "        all_pos.update(positives_shuffled)\n",
    "                \n",
    "        positives_small = list(combinations(small, 2))\n",
    "        positives_selected_small = [x for x in positives_shuffled if (x[0],x[1]) in positives_small or (x[1],x[0]) in positives_small]\n",
    "        all_pos_small.update(positives_selected_small)\n",
    "        \n",
    "        positives_medium = list(combinations(medium, 2))\n",
    "        positives_selected_medium = [x for x in positives_shuffled if (x[0],x[1]) in positives_medium or (x[1],x[0]) in positives_medium]\n",
    "        all_pos_medium.update(positives_selected_medium)\n",
    "        \n",
    "    large = all_pos | all_neg\n",
    "    medium = all_pos_medium | all_neg_medium\n",
    "    small = all_pos_small | all_neg_small\n",
    "    \n",
    "    return small, medium, large\n",
    "\n",
    "# function to build the test split from the test offers\n",
    "def build_test(corpus, ids):\n",
    "    \n",
    "    corpus = corpus.copy()\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces]\n",
    "    cids = set(corpus['cluster_id'])\n",
    "    \n",
    "    corpus['title_processed'] = corpus['title'].apply(lower_to_unicode).copy()\n",
    "    corpus['title_processed'] = corpus['title_processed'].apply(preprocess_string, args=(CUSTOM_FILTERS,))\n",
    "    \n",
    "    all_pos = set()\n",
    "    all_neg = set()\n",
    "    \n",
    "    for cid in tqdm(cids):\n",
    "        sub_corpus = corpus[corpus['cluster_id'] == cid]\n",
    "        sub_corpus_wo = corpus[~(corpus['cluster_id'] == cid)]\n",
    "        \n",
    "        similarities=[sm.SoftTfIdf(corpus_list=sub_corpus_wo['title_processed'].tolist()), sm.Cosine()]\n",
    "        \n",
    "        positives = list(combinations(sub_corpus['id'].tolist(), 2))\n",
    "        random.seed(42)\n",
    "        positives_shuffled = [random.sample([(x[0],x[1]),(x[1], x[0])],1)[0] for x in positives]\n",
    "\n",
    "        all_pos.update(positives_shuffled)\n",
    "        \n",
    "        for i, row in sub_corpus.iterrows():\n",
    "            \n",
    "            example = row['title_processed']\n",
    "            similarities_results = []\n",
    "            for sim in similarities:\n",
    "                try:\n",
    "                    result = sub_corpus_wo['title_processed'].apply(lambda x: sim.get_sim_score(example, x))\n",
    "                except AttributeError:\n",
    "                    result = sub_corpus_wo['title_processed'].apply(lambda x: sim.get_raw_score(example, x))\n",
    "                similarities_results.append(result)\n",
    "            sorted_sim = [x.sort_values(ascending=False) for x in similarities_results]\n",
    "            \n",
    "            selected_negs = set()\n",
    "            already_selected_clusters = set()\n",
    "            ids_to_remove = set()\n",
    "            \n",
    "            for run, similarity in enumerate(sorted_sim):\n",
    "                for idx in similarity.index:\n",
    "                        \n",
    "                    if len(selected_negs) == 2 and run == 0:\n",
    "                        break\n",
    "                    elif len(selected_negs) == 3 and run == 1:\n",
    "                        break\n",
    "\n",
    "                    rel_clu = sub_corpus_wo.loc[idx]['cluster_id']\n",
    "\n",
    "                    if rel_clu not in already_selected_clusters and (i,idx) not in all_neg and (idx,i) not in all_neg:\n",
    "                        random.seed(42)\n",
    "                        selected_negs.update(random.sample([(i,idx), (idx,i)],1))\n",
    "                        ids_to_remove.add(idx)\n",
    "                        already_selected_clusters.add(rel_clu)\n",
    "            \n",
    "            all_neg.update(selected_negs)\n",
    "            cur_length = len(all_neg)\n",
    "            rnd_sample_corpus = sub_corpus_wo.loc[~sub_corpus_wo.index.isin(ids_to_remove)]\n",
    "            cur_seed = 42\n",
    "            while len(all_neg) == cur_length:\n",
    "                rnd_id = rnd_sample_corpus.sample(1, random_state=cur_seed)['id'].iloc[0]\n",
    "                cur_seed += 1\n",
    "                if (i, rnd_id) not in all_neg and (rnd_id, i) not in all_neg:\n",
    "                    random.seed(42)\n",
    "                    all_neg.update(random.sample([(i, rnd_id), (rnd_id, i)], 1))\n",
    "            \n",
    "    all_ids = all_pos | all_neg\n",
    "    \n",
    "    return all_ids\n",
    "\n",
    "def generate_pairs(id_pairs, corpus):\n",
    "    id_pairs = list(id_pairs)\n",
    "    corpus = corpus[['id', 'brand', 'title', 'description', 'price', 'priceCurrency',\n",
    "       'specTableContent', 'cluster_id']]\n",
    "    left_ids, right_ids = list(zip(*id_pairs))\n",
    "\n",
    "    left_offers = corpus.loc[left_ids,:]\n",
    "    right_offers = corpus.loc[right_ids,:]\n",
    "    \n",
    "    left_offers = left_offers.reset_index(drop=True)\n",
    "    right_offers = right_offers.reset_index(drop=True)\n",
    "    \n",
    "    joined = left_offers.join(right_offers, lsuffix='_left', rsuffix='_right')\n",
    "    joined['pair_id'] = joined['id_left'].astype(str) + '#' + joined['id_right'].astype(str)\n",
    "    joined['label'] = joined['cluster_id_left'] == joined['cluster_id_right']\n",
    "    joined['label'] = joined['label'].astype(int)\n",
    "    return joined\n",
    "\n",
    "def generate_multiclass(pairwise_set, corpus):\n",
    "    ids = set()\n",
    "    pairwise_set = pairwise_set[pairwise_set['label'] == 1]\n",
    "    corpus = corpus[['id', 'brand', 'title', 'description', 'price', 'priceCurrency',\n",
    "       'specTableContent', 'cluster_id']]\n",
    "    ids.update(pairwise_set['id_left'])\n",
    "    ids.update(pairwise_set['id_right'])\n",
    "    ids = list(ids)\n",
    "    multiclass_set = corpus.loc[ids,:]\n",
    "    multiclass_set['label'] = multiclass_set['cluster_id']\n",
    "    multiclass_set = multiclass_set.reset_index(drop=True)\n",
    "    \n",
    "    return multiclass_set\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d30e48",
   "metadata": {},
   "source": [
    "# Load cleansed PDC2020 corpus and split into seen and unseen candidates given labeled DBSCAN clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b505e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_pickle('../../../data/interim/wdc-lspc/corpus/dedup_preprocessed_lspcV2020_only_en_strict_only_long_title_only_mainentity.pkl.gz')\n",
    "print(len(corpus))\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_candidates_seen = pd.read_csv('../../../data/interim/wdc-lspc/corpus/seen_dbscan_clusters_annotated.csv')\n",
    "cc_candidates_seen = cc_candidates_seen[cc_candidates_seen['good'] == 1]\n",
    "cc_candidates_seen = sorted(list(set(cc_candidates_seen['dbscan_cluster'])))\n",
    "\n",
    "cc_candidates_unseen = pd.read_csv('../../../data/interim/wdc-lspc/corpus/unseen_dbscan_clusters_annotated.csv')\n",
    "cc_candidates_unseen = cc_candidates_unseen[cc_candidates_unseen['good'] == 1]\n",
    "cc_candidates_unseen = sorted(list(set(cc_candidates_unseen['dbscan_cluster'])))\n",
    "\n",
    "dbscan_mapping = pd.read_csv('../../../data/interim/wdc-lspc/corpus/seen_dbscan_mapping.csv')\n",
    "corpus_dbscan = corpus.merge(dbscan_mapping, how='outer', on='cluster_id').copy()\n",
    "\n",
    "dbscan_mapping_unseen = pd.read_csv('../../../data/interim/wdc-lspc/corpus/unseen_dbscan_mapping.csv')\n",
    "corpus_dbscan_unseen = corpus.merge(dbscan_mapping_unseen, how='outer', on='cluster_id').copy()\n",
    "\n",
    "counts = corpus_dbscan['cluster_id'].value_counts()\n",
    "counts = counts[counts > 6]\n",
    "corpus_dbscan = corpus_dbscan[corpus_dbscan['cluster_id'].isin(counts.index)]\n",
    "\n",
    "counts_unseen = corpus_dbscan_unseen['cluster_id'].value_counts()\n",
    "counts_unseen = counts_unseen[counts_unseen > 3]\n",
    "counts_unseen = counts_unseen[counts_unseen < 7]\n",
    "corpus_dbscan_unseen = corpus_dbscan_unseen[corpus_dbscan_unseen['cluster_id'].isin(counts_unseen.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d82858b",
   "metadata": {},
   "source": [
    "# select hard negative clusters for all three difficulties, 80%, 50% and 20% as well as complentary random clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs_large, ccs_medium, ccs_small = select_cc_clusters(corpus_dbscan, cc_candidates_seen)\n",
    "ccs_large_unseen, ccs_medium_unseen, ccs_small_unseen = select_cc_clusters(corpus_dbscan_unseen, cc_candidates_unseen)\n",
    "rnd_large, rnd_medium, rnd_small = select_rnd_clusters(corpus_dbscan, ccs_large, cc_candidates_seen)\n",
    "rnd_large_unseen, rnd_medium_unseen, rnd_small_unseen = select_rnd_clusters(corpus_dbscan_unseen, ccs_large_unseen, cc_candidates_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8107697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ensure larger sets contain all clusters from smaller sets\n",
    "assert len(ccs_small & ccs_medium) == 20\n",
    "assert len(ccs_small & ccs_large) == 20\n",
    "assert len(ccs_medium & ccs_large) == 50\n",
    "\n",
    "assert len(rnd_small & rnd_medium) == 100\n",
    "assert len(rnd_small & rnd_large) == 100\n",
    "assert len(rnd_medium & rnd_large) == 250\n",
    "\n",
    "assert len(ccs_small_unseen & ccs_medium_unseen) == 20\n",
    "assert len(ccs_small_unseen & ccs_large_unseen) == 20\n",
    "assert len(ccs_medium_unseen & ccs_large_unseen) == 50\n",
    "\n",
    "assert len(rnd_small_unseen & rnd_medium_unseen) == 100\n",
    "assert len(rnd_small_unseen & rnd_large_unseen) == 100\n",
    "assert len(rnd_medium_unseen & rnd_large_unseen) == 250\n",
    "\n",
    "ccs_unpack_large = set()\n",
    "ccs_unpack_large.update(*ccs_large)\n",
    "\n",
    "ccs_unpack_medium = set()\n",
    "ccs_unpack_medium.update(*ccs_medium)\n",
    "\n",
    "ccs_unpack_small = set()\n",
    "ccs_unpack_small.update(*ccs_small)\n",
    "\n",
    "ccs_unpack_unseen_large = set()\n",
    "ccs_unpack_unseen_large.update(*ccs_large_unseen)\n",
    "\n",
    "ccs_unpack_unseen_medium = set()\n",
    "ccs_unpack_unseen_medium.update(*ccs_medium_unseen)\n",
    "\n",
    "ccs_unpack_unseen_small = set()\n",
    "ccs_unpack_unseen_small.update(*ccs_small_unseen)\n",
    "\n",
    "assert len((ccs_unpack_small | ccs_unpack_medium | ccs_unpack_large | rnd_small | rnd_medium | rnd_large) & (ccs_unpack_unseen_small | ccs_unpack_unseen_medium | ccs_unpack_unseen_large | rnd_small_unseen | rnd_medium_unseen | rnd_large_unseen)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce8733",
   "metadata": {},
   "source": [
    "# Generate training, validation and test sets for all hardness levels, development sizes and unseen percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce739744",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_80, valid_80, test_80 = build_pairs(pd.concat([corpus_dbscan, corpus_dbscan_unseen]), ccs_large, rnd_small, ccs_large_unseen, rnd_small_unseen)\n",
    "hard = ('80cc20rnd000un',[train_80, valid_80, test_80])\n",
    "\n",
    "train_50, valid_50, test_50 = build_pairs(pd.concat([corpus_dbscan, corpus_dbscan_unseen]), ccs_medium, rnd_medium, ccs_medium_unseen, rnd_medium_unseen)\n",
    "medium = ('50cc50rnd000un',[train_50, valid_50, test_50])\n",
    "\n",
    "train_20, valid_20, test_20 = build_pairs(pd.concat([corpus_dbscan, corpus_dbscan_unseen]), ccs_small, rnd_large, ccs_small_unseen, rnd_large_unseen)\n",
    "easy = ('20cc80rnd000un',[train_20, valid_20, test_20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcea7bea",
   "metadata": {},
   "source": [
    "# Materialize datasets and write them to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for combination in [hard, medium, easy]:\n",
    "    name = combination[0]\n",
    "    train, valid, test = combination[1]\n",
    "\n",
    "    test_000_unseen = generate_pairs(test[0], corpus).sample(frac=1.0, random_state=42)\n",
    "    test_050_unseen = generate_pairs(test[1], corpus).sample(frac=1.0, random_state=42)\n",
    "    test_100_unseen = generate_pairs(test[2], corpus).sample(frac=1.0, random_state=42)\n",
    "\n",
    "    test_multi_000_unseen = generate_multiclass(test_000_unseen, corpus).sample(frac=1.0, random_state=42)\n",
    "    test_multi_050_unseen = generate_multiclass(test_050_unseen, corpus).sample(frac=1.0, random_state=42)\n",
    "    test_multi_100_unseen = generate_multiclass(test_100_unseen, corpus).sample(frac=1.0, random_state=42)\n",
    "\n",
    "    train_small = generate_pairs(train[0], corpus).sample(frac=1.0, random_state=42)\n",
    "    train_medium = generate_pairs(train[1], corpus).sample(frac=1.0, random_state=42)\n",
    "    train_large = generate_pairs(train[2], corpus).sample(frac=1.0, random_state=42)\n",
    "\n",
    "    train_small_multi = generate_multiclass(train_small, corpus).sample(frac=1.0, random_state=42)\n",
    "    train_medium_multi = generate_multiclass(train_medium, corpus).sample(frac=1.0, random_state=42)\n",
    "    train_large_multi = generate_multiclass(train_large, corpus).sample(frac=1.0, random_state=42)\n",
    "\n",
    "    valid_small = generate_pairs(valid[0], corpus).sample(frac=1.0, random_state=42)\n",
    "    valid_medium = generate_pairs(valid[1], corpus).sample(frac=1.0, random_state=42)\n",
    "    valid_large = generate_pairs(valid[2], corpus).sample(frac=1.0, random_state=42)\n",
    "\n",
    "    valid_small_multi = generate_multiclass(valid_small, corpus).sample(frac=1.0, random_state=42)\n",
    "    valid_medium_multi = generate_multiclass(valid_medium, corpus).sample(frac=1.0, random_state=42)\n",
    "    valid_large_multi = generate_multiclass(valid_large, corpus).sample(frac=1.0, random_state=42)\n",
    "    \n",
    "    train_small.to_json(f'../../../data/raw/wdc-lspc/training-sets/wdcproducts{name}_train_small.json.gz', lines=True, orient='records')\n",
    "    train_medium.to_json(f'../../../data/raw/wdc-lspc/training-sets/wdcproducts{name}_train_medium.json.gz', lines=True, orient='records')\n",
    "    train_large.to_json(f'../../../data/raw/wdc-lspc/training-sets/wdcproducts{name}_train_large.json.gz', lines=True, orient='records')\n",
    "    \n",
    "    train_small_multi.to_json(f'../../../data/raw/wdc-lspc/training-sets/wdcproductsvmulti{name}_train_small.json.gz', lines=True, orient='records')\n",
    "    train_medium_multi.to_json(f'../../../data/raw/wdc-lspc/training-sets/wdcproductsmulti{name}_train_medium.json.gz', lines=True, orient='records')\n",
    "    train_large_multi.to_json(f'../../../data/raw/wdc-lspc/training-sets/wdcproductsmulti{name}_train_large.json.gz', lines=True, orient='records')\n",
    "\n",
    "    valid_small.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproducts{name}_valid_small.json.gz', lines=True, orient='records')\n",
    "    valid_medium.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproducts{name}_valid_medium.json.gz', lines=True, orient='records')\n",
    "    valid_large.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproducts{name}_valid_large.json.gz', lines=True, orient='records')\n",
    "    \n",
    "    valid_small_multi.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproductsmulti{name}_valid_small.json.gz', lines=True, orient='records')\n",
    "    valid_medium_multi.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproductsmulti{name}_valid_medium.json.gz', lines=True, orient='records')\n",
    "    valid_large_multi.to_json(f'../../../data/raw/wdc-lspc/validation-sets/wdcproductsmulti{name}_valid_large.json.gz', lines=True, orient='records')\n",
    "    \n",
    "    test_000_unseen.to_json(f'../../../data/raw/wdc-lspc/gold-standards/wdcproducts{name}_gs.json.gz', lines=True, orient='records')\n",
    "    test_multi_000_unseen.to_json(f'../../../data/raw/wdc-lspc/gold-standards/wdcproductsmulti{name}_gs.json.gz', lines=True, orient='records')\n",
    "    \n",
    "    test_050_unseen.to_json(f'../../../data/raw/wdc-lspc/gold-standards/wdcproducts{name.replace(\"000un\", \"050un\")}_gs.json.gz', lines=True, orient='records')\n",
    "    test_multi_050_unseen.to_json(f'../../../data/raw/wdc-lspc/gold-standards/wdcproductsmulti{name.replace(\"000un\", \"050un\")}_gs.json.gz', lines=True, orient='records')\n",
    "    \n",
    "    test_100_unseen.to_json(f'../../../data/raw/wdc-lspc/gold-standards/wdcproducts{name.replace(\"000un\", \"100un\")}_gs.json.gz', lines=True, orient='records')\n",
    "    test_multi_100_unseen.to_json(f'../../../data/raw/wdc-lspc/gold-standards/wdcproducts{name.replace(\"000un\", \"100un\")}_gs.json.gz', lines=True, orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
