{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb69321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import math\n",
    "import string\n",
    "import regex as re\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "import fasttext\n",
    "\n",
    "from gensim.parsing.preprocessing import lower_to_unicode, preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7faa6",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0bdaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# class for fasttext language detection\n",
    "class LanguageIdentification:\n",
    "\n",
    "    def __init__(self):\n",
    "        pretrained_lang_model = '../../../fasttext/lid.176.bin'\n",
    "        self.model = fasttext.load_model(pretrained_lang_model)\n",
    "\n",
    "    def predict_lang(self, text):\n",
    "        predictions = self.model.predict(text, k=5) # returns top 5 matching languages\n",
    "        return predictions\n",
    "\n",
    "# function to apply fasttext model and check for less than 5 non-latin chars\n",
    "def assign_language(text):\n",
    "    \n",
    "    processed_text = lower_to_unicode(text)\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric]\n",
    "    \n",
    "    processed_text = ' '.join(preprocess_string(processed_text, CUSTOM_FILTERS))\n",
    "    \n",
    "    if processed_text.strip() == '':\n",
    "        return False\n",
    "    \n",
    "    result = LANGUAGE.predict_lang(processed_text)\n",
    "    \n",
    "    langs = [x.split('__')[-1] for x in result[0]]\n",
    "    probs = result[1]\n",
    "    \n",
    "    if 'en' not in langs:\n",
    "        return False\n",
    "    en_idx = langs.index('en')\n",
    "    if probs[en_idx] == max(probs):\n",
    "        if len(set(re.findall(r'(\\p{IsArabic}|\\p{IsArmenian}|\\p{IsBengali}|\\p{IsBopomofo}|\\p{IsBraille}|\\p{IsBuhid}|\\p{IsCanadian_Aboriginal}|\\p{IsCherokee}|\\p{IsCyrillic}|\\p{IsDevanagari}|\\p{IsEthiopic}|\\p{IsGeorgian}|\\p{IsGreek}|\\p{IsGujarati}|\\p{IsGurmukhi}|\\p{IsHan}|\\p{IsHangul}|\\p{IsHanunoo}|\\p{IsHebrew}|\\p{IsHiragana}|\\p{IsKannada}|\\p{IsKatakana}|\\p{IsKhmer}|\\p{IsLao}|\\p{IsLimbu}|\\p{IsMalayalam}|\\p{IsMongolian}|\\p{IsMyanmar}|\\p{IsOgham}|\\p{IsOriya}|\\p{IsRunic}|\\p{IsSinhala}|\\p{IsSyriac}|\\p{IsTagalog}|\\p{IsTagbanwa}|\\p{IsTaiLe}|\\p{IsTamil}|\\p{IsTelugu}|\\p{IsThaana}|\\p{IsThai}|\\p{IsTibetan}|\\p{IsYi})', processed_text))) > 4:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# function to remove titles with length less than 5 words\n",
    "def cleanse_by_length(text):\n",
    "    \n",
    "    processed_text = lower_to_unicode(text)\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces]\n",
    "    \n",
    "    processed_text = preprocess_string(processed_text, CUSTOM_FILTERS)\n",
    "    \n",
    "    if len(processed_text) < 5:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# function for cluster-based cleansing\n",
    "def cleanse_by_mainentity_heuristic(cur_cluster):\n",
    "    \n",
    "    cur_cluster = cur_cluster.set_index('id', drop=False)\n",
    "    cluster_len = len(cur_cluster)\n",
    "    word_dict = {}\n",
    "    lengths = []\n",
    "    to_remove = []\n",
    "\n",
    "    for i, v in cur_cluster['title_processed'].iteritems():\n",
    "\n",
    "        words = v\n",
    "        lengths.append(len(words))\n",
    "\n",
    "        for word in words:\n",
    "            if word in word_dict.keys():\n",
    "                word_dict[word] += 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "\n",
    "    for i, v in cur_cluster['title_processed'].iteritems():\n",
    "\n",
    "        count_overlap_main = 0\n",
    "        \n",
    "        words = v\n",
    "        for word in words:\n",
    "            if word_dict[word] >= math.ceil(cluster_len / 3):\n",
    "                count_overlap_main += 1\n",
    "        if count_overlap_main <= math.floor((sum(lengths)/len(lengths)) / 4):\n",
    "            to_remove.append(i)\n",
    "\n",
    "    return to_remove\n",
    "    \n",
    "LANGUAGE = LanguageIdentification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ebfbb9",
   "metadata": {},
   "source": [
    "# Language Identification and Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c8283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8545276d7ad431d8407ea7a629b0242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98900648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../data/interim/wdc-lspc/corpus/preprocessed_lspcV2020.pkl.gz')\n",
    "corpus['title_temp'] = corpus['title'].fillna('')\n",
    "corpus['description_temp'] = corpus['description'].fillna('')\n",
    "corpus['title+desc'] = corpus['title_temp'] + ' ' + corpus['description_temp']\n",
    "corpus['is_en'] = corpus['title+desc'].progress_apply(assign_language)\n",
    "\n",
    "corpus = corpus[['id', 'brand', 'title', 'description', 'price', 'priceCurrency', 'specTableContent', 'cluster_id', \n",
    "                  'sku', 'mpn', \n",
    "                  'gtin', 'gtin8', 'gtin12', 'gtin13', 'gtin14', 'productID', 'identifier', 'is_en']]\n",
    "corpus.to_pickle('../../../data/interim/wdc-lspc/corpus/preprocessed_lspcV2020_only_en_strict.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba147924",
   "metadata": {},
   "source": [
    "# Deduplication on titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4033782",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus[corpus['is_en'] == True]\n",
    "corpus['description_temp'] = corpus['description']\n",
    "corpus['brand_temp'] = corpus['brand']\n",
    "corpus['specTableContent_temp'] = corpus['specTableContent']\n",
    "corpus = corpus.dropna(subset=['title'])\n",
    "corpus = corpus.fillna({'description_temp':'', 'brand_temp':'', 'specTableContent_temp':''})\n",
    "\n",
    "corpus['title+description+brand+specTableContent'] = corpus['title'] + ' ' + corpus['description_temp'] + ' ' + corpus['brand_temp'] + ' ' + corpus['specTableContent_temp']\n",
    "corpus['title+description+brand+specTableContent'] = corpus['title+description+brand+specTableContent'].apply(lambda x: ' '.join(x.lower().split()))\n",
    "corpus = corpus.drop_duplicates(subset=['title+description+brand+specTableContent'])\n",
    "\n",
    "corpus = corpus[['id', 'brand', 'title', 'description', 'price', 'priceCurrency', 'specTableContent', 'cluster_id', \n",
    "                  'sku', 'mpn', \n",
    "                  'gtin', 'gtin8', 'gtin12', 'gtin13', 'gtin14', 'productID', 'identifier', 'is_en']]\n",
    "\n",
    "corpus.to_pickle('../../../data/interim/wdc-lspc/corpus/dedup_preprocessed_lspcV2020_only_en_strict.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18559a",
   "metadata": {},
   "source": [
    "# Remove short titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee70891",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['has_long_title'] = corpus['title'].progress_apply(cleanse_by_length)\n",
    "corpus = corpus[corpus['has_long_title'] == True]\n",
    "corpus = corpus[['id', 'brand', 'title', 'description', 'price', 'priceCurrency', 'specTableContent', 'cluster_id', \n",
    "                  'sku', 'mpn', \n",
    "                  'gtin', 'gtin8', 'gtin12', 'gtin13', 'gtin14', 'productID', 'identifier', 'is_en', 'has_long_title']]\n",
    "corpus.to_pickle('../../../data/interim/wdc-lspc/corpus/dedup_preprocessed_lspcV2020_only_en_strict_only_long_title.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cbc3c6",
   "metadata": {},
   "source": [
    "# Apply cluster-based cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b132f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "corpus['is_mainentity'] = True\n",
    "    \n",
    "corpus_selection = corpus[['title', 'cluster_id', 'id']].copy()\n",
    "counts = corpus_selection['cluster_id'].value_counts()\n",
    "counts = counts[counts > 1]\n",
    "corpus_selection = corpus_selection[corpus_selection['cluster_id'].isin(counts.index)]\n",
    "\n",
    "clusters = list(set(corpus_selection['cluster_id']))\n",
    "to_remove = []\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces]\n",
    "\n",
    "corpus_selection['title_processed'] = corpus_selection['title'].apply(lower_to_unicode)\n",
    "corpus_selection['title_processed'] = corpus_selection['title_processed'].apply(preprocess_string, args=(CUSTOM_FILTERS,))\n",
    "corpus_selection['title_processed'] = corpus_selection['title_processed'].apply(set)\n",
    "corpus_selection = corpus_selection[['title_processed', 'cluster_id', 'id']]\n",
    "\n",
    "corpus_selection = corpus_selection.set_index('cluster_id', drop=False)\n",
    "corpus_selection = corpus_selection.sort_index()\n",
    "\n",
    "clusters_data = [corpus_selection.loc[cluster] for cluster in tqdm(clusters)]\n",
    "\n",
    "batch = 10000\n",
    "n_workers = 20\n",
    "\n",
    "results = process_map(cleanse_by_mainentity_heuristic,clusters_data, max_workers=n_workers, chunksize=batch)\n",
    "\n",
    "to_remove = set()\n",
    "\n",
    "for result in tqdm(results):\n",
    "    to_remove.update(result)\n",
    "    \n",
    "corpus.loc[to_remove, 'is_mainentity'] = False\n",
    "\n",
    "corpus = corpus[corpus['is_mainentity'] == True]\n",
    "corpus = corpus[['id', 'brand', 'title', 'description', 'price', 'priceCurrency', 'specTableContent', 'cluster_id', \n",
    "                  'sku', 'mpn', \n",
    "                  'gtin', 'gtin8', 'gtin12', 'gtin13', 'gtin14', 'productID', 'identifier', 'is_en', 'has_long_title', 'is_mainentity']]\n",
    "corpus.to_pickle('../../../data/interim/wdc-lspc/corpus/dedup_preprocessed_lspcV2020_only_en_strict_only_long_title_only_mainentity.pkl.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
